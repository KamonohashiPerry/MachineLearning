{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Python_Causal_Inference_Chap8.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMP0KdqCW3W+WMySG8ypSVw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KamonohashiPerry/MachineLearning/blob/master/Causal_Inference/Python_Causal_Inference_Chap8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLpbzeIt7SDz",
        "colab_type": "text"
      },
      "source": [
        "## SAM(Structural Agnostic Model)\n",
        "+ 識別器Dのネットワーク構造\n",
        " + バッチノーマライゼーション\n",
        "+ 生成器G\n",
        " + 1次元のバッチノーマライゼーション\n",
        "  + 複数個の変数の値を一気に生成するのではなく、ある一つを生成し、残りは観測データを与えて、一つだけ生成するようにする。（ギブスサンプリングみたいな感じ）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyDSGkVz6uaM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "3caf7d37-e41b-467d-dee6-4b6ea2f5ac09"
      },
      "source": [
        "# PyTorchのバージョンを下げる\n",
        "!pip install torch==1.4.0+cu92 torchvision==0.5.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.4.0+cu92\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu92/torch-1.4.0%2Bcu92-cp36-cp36m-linux_x86_64.whl (640.5MB)\n",
            "\u001b[K     |████████████████████████████████| 640.6MB 22kB/s \n",
            "\u001b[?25hCollecting torchvision==0.5.0+cu92\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu92/torchvision-0.5.0%2Bcu92-cp36-cp36m-linux_x86_64.whl (3.9MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0MB 18.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0+cu92) (7.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0+cu92) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0+cu92) (1.18.5)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.6.0+cu101\n",
            "    Uninstalling torch-1.6.0+cu101:\n",
            "      Successfully uninstalled torch-1.6.0+cu101\n",
            "  Found existing installation: torchvision 0.7.0+cu101\n",
            "    Uninstalling torchvision-0.7.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.7.0+cu101\n",
            "Successfully installed torch-1.4.0+cu92 torchvision-0.5.0+cu92\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwPkJsEv-0Jw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2d015334-090f-44f2-e902-5c26440d30a6"
      },
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.4.0+cu92\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFw7zseU-9FT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "outputId": "8a96ca28-bd09-41d7-deb9-131461dcc390"
      },
      "source": [
        "!pip install cdt==0.5.18"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting cdt==0.5.18\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/29/144be44af187c8a2af63ceb205c38ca11787589f532cdf76517333d92d90/cdt-0.5.18-py3-none-any.whl (917kB)\n",
            "\u001b[K     |████████████████████████████████| 921kB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from cdt==0.5.18) (2.23.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from cdt==0.5.18) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from cdt==0.5.18) (1.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from cdt==0.5.18) (2.4)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.6/dist-packages (from cdt==0.5.18) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from cdt==0.5.18) (4.41.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from cdt==0.5.18) (1.0.5)\n",
            "Collecting GPUtil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from cdt==0.5.18) (1.18.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from cdt==0.5.18) (0.16.0)\n",
            "Collecting skrebate\n",
            "  Downloading https://files.pythonhosted.org/packages/d3/8a/969e619753c299b4d3943808ef5f7eb6587d3cb78c93dcbcc3e4ce269f89/skrebate-0.61.tar.gz\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->cdt==0.5.18) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->cdt==0.5.18) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->cdt==0.5.18) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->cdt==0.5.18) (1.24.3)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->cdt==0.5.18) (4.4.2)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from statsmodels->cdt==0.5.18) (0.5.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->cdt==0.5.18) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->cdt==0.5.18) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from patsy>=0.4.0->statsmodels->cdt==0.5.18) (1.15.0)\n",
            "Building wheels for collected packages: GPUtil, skrebate\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7413 sha256=c771880ce4a5dca99f647cc20196d248e728b48083df90fc1ad8e4464e17d05f\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "  Building wheel for skrebate (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for skrebate: filename=skrebate-0.61-cp36-none-any.whl size=29257 sha256=7d83aa5edc91eee2642e08d5819ce211730d538d48914a0ac3f970f1c70ae75e\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/d8/ae/9b51d487e9d02219996d6c260255a216ef07d905b0a0b00ce3\n",
            "Successfully built GPUtil skrebate\n",
            "Installing collected packages: GPUtil, skrebate, cdt\n",
            "Successfully installed GPUtil-1.4.0 cdt-0.5.18 skrebate-0.61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTw21jw6_QQ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 乱数のシードを設定\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(1234)\n",
        "random.seed(1234)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqq2bZQr_bMV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "661d8a98-dd76-4e8a-aaf1-281bbe799f0f"
      },
      "source": [
        "# 使用するパッケージ（ライブラリと関数）を定義\n",
        "# 標準正規分布の生成用\n",
        "from numpy.random import *\n",
        "\n",
        "# グラフの描画用\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# その他\n",
        "import pandas as pd\n",
        "\n",
        "# シグモイド関数をimport\n",
        "from scipy.special import expit\n",
        "\n",
        "# データ数\n",
        "num_data = 2000\n",
        "\n",
        "# 部下育成への熱心さ\n",
        "x = np.random.uniform(low=-1, high=1, size=num_data)  # -1から1の一様乱数\n",
        "\n",
        "# 上司が「上司向け：部下とのキャリア面談のポイント研修」に参加したかどうか\n",
        "e_z = randn(num_data)  # ノイズの生成\n",
        "z_prob = expit(-5.0*x+5*e_z)\n",
        "Z = np.array([])\n",
        "\n",
        "# 上司が「上司向け：部下とのキャリア面談のポイント研修」に参加したかどうか\n",
        "for i in range(num_data):\n",
        "    Z_i = np.random.choice(2, size=1, p=[1-z_prob[i], z_prob[i]])[0]\n",
        "    Z = np.append(Z, Z_i)\n",
        "\n",
        "# 介入効果の非線形性：部下育成の熱心さxの値に応じて段階的に変化\n",
        "t = np.zeros(num_data)\n",
        "for i in range(num_data):\n",
        "    if x[i] < 0:\n",
        "        t[i] = 0.5\n",
        "    elif x[i] >= 0 and x[i] < 0.5:\n",
        "        t[i] = 0.7\n",
        "    elif x[i] >= 0.5:\n",
        "        t[i] = 1.0\n",
        "\n",
        "e_y = randn(num_data)\n",
        "Y = 2.0 + t*Z + 0.3*x + 0.1*e_y \n",
        "\n",
        "\n",
        "# 本章からの追加データを生成\n",
        "\n",
        "# Y2：部下当人のチームメンバへの満足度 1から5の5段階\n",
        "Y2 = np.random.choice([1.0, 2.0, 3.0, 4.0, 5.0],\n",
        "                      num_data, p=[0.1, 0.2, 0.3, 0.2, 0.2])\n",
        "\n",
        "# Y3：部下当人の仕事への満足度\n",
        "e_y3 = randn(num_data)\n",
        "Y3 = 3*Y + Y2 + e_y3\n",
        "\n",
        "# Y4：部下当人の仕事のパフォーマンス\n",
        "e_y4 = randn(num_data)\n",
        "Y4 = 3*Y3 + 2*e_y4 + 5\n",
        "\n",
        "df = pd.DataFrame({'x': x,\n",
        "                   'Z': Z,\n",
        "                   't': t,\n",
        "                   'Y': Y,\n",
        "                   'Y2': Y2,\n",
        "                   'Y3': Y3,\n",
        "                   'Y4': Y4,\n",
        "                   })\n",
        "\n",
        "del df[\"t\"]  # 変数tは観測できないので削除\n",
        "\n",
        "df.head()  # 先頭を表示"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x</th>\n",
              "      <th>Z</th>\n",
              "      <th>Y</th>\n",
              "      <th>Y2</th>\n",
              "      <th>Y3</th>\n",
              "      <th>Y4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.807894</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.291971</td>\n",
              "      <td>3.0</td>\n",
              "      <td>10.087388</td>\n",
              "      <td>33.942040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.819267</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.169256</td>\n",
              "      <td>2.0</td>\n",
              "      <td>9.693777</td>\n",
              "      <td>36.705939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.907815</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.247729</td>\n",
              "      <td>4.0</td>\n",
              "      <td>9.710220</td>\n",
              "      <td>33.767494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.033905</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.543262</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9.692669</td>\n",
              "      <td>36.801544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.060081</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.924453</td>\n",
              "      <td>4.0</td>\n",
              "      <td>10.077395</td>\n",
              "      <td>33.666739</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          x    Z         Y   Y2         Y3         Y4\n",
              "0 -0.807894  1.0  2.291971  3.0  10.087388  33.942040\n",
              "1  0.819267  0.0  2.169256  2.0   9.693777  36.705939\n",
              "2 -0.907815  1.0  2.247729  4.0   9.710220  33.767494\n",
              "3 -0.033905  1.0  2.543262  1.0   9.692669  36.801544\n",
              "4 -0.060081  0.0  1.924453  4.0  10.077395  33.666739"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wM7qfp-7_xZI",
        "colab_type": "text"
      },
      "source": [
        "### 識別器Dの実装"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ss9IGYvU_tjF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PyTorchから使用するものをimport\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SAMDiscriminator(nn.Module):\n",
        "  def __init__(self, nfeatures, dnh, hlayers):\n",
        "    super.nfeatures = nfeatures # 入力変数の数\n",
        "    layers = []\n",
        "    layers.append(nn.Linear(nfeatures, dnh))\n",
        "    layers.append(nn.BatchNorm1d(dnh))\n",
        "    layers.append(nn.LeakyReLU(.2))\n",
        "\n",
        "    for i in range(hlayers - 1):\n",
        "      layers.append(nn.Linear(dnh, dnh))\n",
        "      layers.append(nn.BatchNorm1d(dnh))\n",
        "      layers.append(nn.LeakyReLU(.2))\n",
        "\n",
        "    layers.append(nn.Linear(dnh, 1))\n",
        "\n",
        "    self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    # mask\n",
        "    mask = torch.eye(nfeatures, nfeatures)\n",
        "    self.register_buffer('mask', mask.unsqueeze(0))\n",
        "\n",
        "  def forward(self, input, obs_data=None):\n",
        "    # 順伝播の計算\n",
        "    if obs_data is not None:\n",
        "      # 生成データを識別器に入力する場合\n",
        "      return [self.layesr(i) for i in torch.unbind(obs_data.unsqueeze(1) * (1-self.mask) + input.unsqueeze(1)*self.mask, 1) ]\n",
        "\n",
        "    else:\n",
        "      # 観測データを識別器に入力する場合\n",
        "      return self.layers(input)\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    # 識別器Dの重みパラメータの初期化\n",
        "    for layer in self.layers:\n",
        "      if hasattr(layer, 'reset_parameters'):\n",
        "        layer.reset_parameters()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LioG1-tDbHM",
        "colab_type": "text"
      },
      "source": [
        "### 生成器Gの実装"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQtaushdDNNw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from cdt.utils.torch import ChannelBatchNorm1d, MatrixSampler, Linear3D\n",
        "\n",
        "class SAMGenerator(nn.Module):\n",
        "  def __init__(self, data_shape, nh):\n",
        "    # 初期化\n",
        "    super(SAMGenerator, self).__init__()\n",
        "\n",
        "    nb_vars = data_shape[1]\n",
        "    skeleton = 1 - torch.eye(nb_vars + 1, nb_vars)\n",
        "\n",
        "    self.register_buffer('skeleton', skeleton)\n",
        "\n",
        "    # ネットワークの用意\n",
        "    ## 入力層\n",
        "    self.input_layer = Linear3D((nb_vars, nb_vars + 1, nh))\n",
        "\n",
        "    ## 中間層\n",
        "    layers = []\n",
        "    # 2次元を1次元に変換してバッチノーマライゼーションする\n",
        "    layers.append(ChannelBatchNorm1d(nb_vars, nh))\n",
        "    layers.append(nn.Tanh())\n",
        "    self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    ## 出力層\n",
        "    self.output_layer = Linear3D((nb_vars, nh, 1))\n",
        "\n",
        "  def forward(self, data, noise, adj_matrix, drawn_neurons=None):\n",
        "    # 順伝播の計算\n",
        "    ## 入力層\n",
        "    x = self.input_layer(data, noise, adj_matrix * self.skeleton)\n",
        "\n",
        "    ## 中間層\n",
        "    x = self.layers(x)\n",
        "\n",
        "    ## 出力層\n",
        "    output = self.output_layer(x, noise=None, adj_matrix=drawn_neurons)\n",
        "\n",
        "    return output.squeeze(2)\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    # 重みパラメータの初期化を実施\n",
        "    self.input_layer.reset_parameters()\n",
        "    self.output_layer.reset_parameters()\n",
        "\n",
        "    for layer in self.layers:\n",
        "      if hasattr(layer, 'reset_parameters'):\n",
        "        layer.reset_parameters()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N95KDheFHiLh",
        "colab_type": "text"
      },
      "source": [
        "## SAMの損失関数\n",
        "+ DAGを生み出す損失関数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CkRyntbFTWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ネットワークを示す因果構造マトリクスMがDAGになるように加える損失\n",
        "def notears_constr(adj_m, max_pow=None):\n",
        "  m_exp = [adj_m]\n",
        "  if max_pow is None:\n",
        "    max_pow = adj_m.shape[1]\n",
        "  while(m_exp[-1].sum() > 0 and len(m_exp) < max_pow ):\n",
        "    m_exp.append(m_exp[-1] @ adj_m/len(m_exp))\n",
        "\n",
        "  return sum([i.diag().sum() for idx, i in enumerate(m_exp)])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYmBppbsJR1e",
        "colab_type": "text"
      },
      "source": [
        "### SAMの学習の実行"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwpCEXB4Icei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import scale\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "def run_SAM(in_data, lr_gen, lr_disc, lambda1, lambda2, hlayers, nh, dnh, train_epochs, test_epochs, device):\n",
        "  # SAMの学習の実行\n",
        "  ## 入力データの前処理\n",
        "  list_nodes = list(in_data.columns)\n",
        "  data = scale(in_data[list_nodes].values)\n",
        "  nb_var = len(list_nodes)\n",
        "  data = data.astype('float32')\n",
        "  data = torch.from_numpy(data).to(device)\n",
        "  rows, cols = data.size()\n",
        "\n",
        "  # DataLoader\n",
        "  batch_size = rows\n",
        "  data_iterator = DataLoader(data, batch_size=batch_size,\n",
        "                              shuffle=True, drop_last=True)\n",
        "  \n",
        "  # ネットワークの生成とパラメータの初期化\n",
        "  sam = SAMGenerator((batch_size, cols), nh).to(device)\n",
        "  graph_sampler = MatrixSampler(nb_var, mask=None, gumber=False).to(device)\n",
        "  neuron_sampler = MatrixSampler((nh, nb_var), mask=False, gumbel=True)\n",
        "\n",
        "  # 重みパラメータの初期化\n",
        "  sam.reset_parameters()\n",
        "  graph_sampler.weights.data.fill_(2)\n",
        "\n",
        "  # ネットワークの生成とパラメータの初期化\n",
        "  discriminator = SAMDiscriminator(cols, dnh, hlayerrs).to(device)\n",
        "  discriminator.reset_parameters()\n",
        "\n",
        "  # 最適化の設定\n",
        "  ## 生成器\n",
        "  g_optimizer = optim.Adam(sam.parameters(), lr=lr_gen)\n",
        "  graph_optimizer = optim.Adam(graph_sampler.parameters(), lr=lr_gen)\n",
        "  neuron_optimizer = optim.Adam(neuron_sampler.parameters(), lr=lr_gen)\n",
        "\n",
        "  ## 識別器\n",
        "  d_optimizer = optim.Adam(discriminator.parameters(), lr=lr_disc)\n",
        "\n",
        "  ## 損失関数\n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "  ## 損失関数のDAGに関する制約の設定パラメータ\n",
        "  dagstart = 0.5\n",
        "  dagpenalization_increase = 0.001*10\n",
        "\n",
        "  ## forward計算、損失関数の計算に使用する変数\n",
        "  _true = torch.ones(1).to(device)\n",
        "  _false = torch.zeros(1).to(device)\n",
        "\n",
        "  noise = torch.randn(batch_size, nb_var).to(device)\n",
        "  noise_row = torch.ones(1, nb_var).to(device)\n",
        "\n",
        "  output = torch.zeros(nb_var, nb_var).to(device)\n",
        "  output_loss = torch.zeros(1, 1).to(device)\n",
        "  \n",
        "  ## forward計算、ネットワーク学習\n",
        "  pbar = tqdm(range(train_epochs + test_epochs))\n",
        "\n",
        "  for epoch in pbar:\n",
        "    for i_batch, batch in enumerate(data_iterator):\n",
        "      # 最適化を初期化\n",
        "      g_optimizer.zero_grad()\n",
        "      graph_optimizer.zero_grad()\n",
        "      neuron_optimizer.zero_grad()\n",
        "      d_optimizer.zero_grad()\n",
        "\n",
        "      # 因果構造マトリクスと複雑さマトリクスの取得\n",
        "      drawn_graph = graph_samplet()\n",
        "      drawn_neurons = neuron_sampler()\n",
        "\n",
        "      # ノイズをリセットし、生成器Gで擬似データを生成\n",
        "      noise.normal_()\n",
        "      generated_variables = sam(data=batch, noise=noise,\n",
        "                                  adj_matrix=torch.cat([drawn_graph, noise_row], 0), drawn_neurons=drawn_neurons)\n",
        "      \n",
        "      # 識別器Dで判定\n",
        "      disc_vars_d = discriminator(generated_variables.detach(), batch)\n",
        "      # 観測変数のリスト\n",
        "      disc_vars_g = discriminator(generated_variables, batch)\n",
        "      true_vars_disc = discriminator(batch)\n",
        "\n",
        "      # 損失関数の計算\n",
        "      disc_loss = sum([criterion(gen, _false.expand_as(gen)) for gen in disc_vars_d ])/ nb_var + criterion(true_vars_disc, _true.expand_as(true_vars_disc))\n",
        "\n",
        "      gen_loss = sum([criterion(gen, _true.expand_as(gen)) for gen in disc_vars_g ])\n",
        "\n",
        "      # 識別器Dのバックプロパゲーションとパラメータの更新\n",
        "      if epoch < train_epochs:\n",
        "        disc_loss.backward()\n",
        "        d_optimizer.step()\n",
        "\n",
        "      # 生成器のGの損失の計算の残り\n",
        "      struc_loss = lambda1 / batch_size*drawn_graph.sum()\n",
        "      func_loss = lambda2 / batch_size*drawn_neurons.sum()\n",
        "\n",
        "      regul_loss = gen_loss + func_loss\n",
        "\n",
        "      if epoch <= train_epochs * dagstart:\n",
        "        # epochが基準前のときはDAGになるようにMへのNo Tearsの制限はかけない\n",
        "        loss = gen_loss + regul_loss\n",
        "\n",
        "      else:\n",
        "        # epochが基準後のときはDAGになるようにMへのNo Tearsの制限をかける\n",
        "        filters = graph_sampler.get_proba()\n",
        "        # マトリクスMの要素を取得\n",
        "        dag_constraint = notears_constr(filters*filters)\n",
        "\n",
        "        # 徐々に線形にDAFの正則を強くする\n",
        "        loss = gen_loss + regul_loss + ((epoch-train_epochs*dagstart)*dagpenalization_increase)*dag_constraint\n",
        "\n",
        "      if epoch >= train_epochs:\n",
        "        # testのepochの場合、結果を取得\n",
        "        output.add_(filters.data)\n",
        "        output_loss.add_(gen_loss.data)\n",
        "\n",
        "      else:\n",
        "        # trainのepochの場合、生成器Gのバックプロパゲーションと更新\n",
        "        loss.backward(retain_graph=True)\n",
        "        g_optimizer.step()\n",
        "        graph_optimizer.step()\n",
        "        neuron_optimizer.step()\n",
        "\n",
        "      # 進捗の表示\n",
        "      if epoch % 50 ==0:\n",
        "        pbar.set_postfix(gen=gen_loss.item()/cols, disc=disc_loss.item(),\n",
        "                         regul_loss=regul_loss.item(),\n",
        "                         tot=loss.item())\n",
        "        \n",
        "  return output.cpu().numpy()/test_epochs, output_loss.cpu().numpy()/test_epochs/cols"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TBezv1wT52c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}