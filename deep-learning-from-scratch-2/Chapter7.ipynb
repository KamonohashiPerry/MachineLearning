{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyONxMK80K5gSKzzaCCQSv/u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KamonohashiPerry/MachineLearning/blob/master/deep-learning-from-scratch-2/Chapter7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOqrE8RCpCG4",
        "colab_type": "text"
      },
      "source": [
        "# RNNによる文章生成\n",
        "+ seq2seq\n",
        " + 時系列から時系列へと変換する、新しい構造のニューラルネットワーク\n",
        " + 二つのRNNを組み合わせることで、seq2seqが実装できる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmQfsJ2npy_N",
        "colab_type": "text"
      },
      "source": [
        "## 言語モデルを使った文章生成"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RBg9K9qqVmU",
        "colab_type": "text"
      },
      "source": [
        "### RNNによる文章生成の手順\n",
        "+ 与えられた単語から、次に出現する単語の確率分布を出力\n",
        "+ 確率的に単語を選択していく。\n",
        "+ その作業を繰り返す。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HSQuOfsxYGq",
        "colab_type": "text"
      },
      "source": [
        "### 文書生成の実装"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSUKxzM0WyIm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "d9935ed7-96ab-46bf-c502-9cd32f6b32b7"
      },
      "source": [
        "!git clone https://github.com/oreilly-japan/deep-learning-from-scratch-2.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'deep-learning-from-scratch-2'...\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 378 (delta 0), reused 0 (delta 0), pack-reused 373\n",
            "Receiving objects: 100% (378/378), 7.91 MiB | 5.41 MiB/s, done.\n",
            "Resolving deltas: 100% (210/210), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDji7z0xum6-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "11dfccde-bf94-468c-86fd-9e74e29febb5"
      },
      "source": [
        "cd deep-learning-from-scratch-2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/deep-learning-from-scratch-2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jhh8VfgWuqdK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "396e5c1c-4f0f-4c9c-8201-1e37f58c3f2b"
      },
      "source": [
        "ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mch01\u001b[0m/  \u001b[01;34mch03\u001b[0m/  \u001b[01;34mch05\u001b[0m/  \u001b[01;34mch07\u001b[0m/  \u001b[01;34mcommon\u001b[0m/   LICENSE.md\n",
            "\u001b[01;34mch02\u001b[0m/  \u001b[01;34mch04\u001b[0m/  \u001b[01;34mch06\u001b[0m/  \u001b[01;34mch08\u001b[0m/  \u001b[01;34mdataset\u001b[0m/  README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ELa9Qauuqyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "import numpy as np\n",
        "from common.functions import softmax\n",
        "from ch06.rnnlm import Rnnlm\n",
        "from ch06.better_rnnlm import BetterRnnlm\n",
        "\n",
        "class RnnlmGen(Rnnlm):\n",
        "  def generate(self, start_id, skip_ids=None, sample_size=100):\n",
        "    word_ids = [start_id] # 最初に与える単語のid\n",
        "\n",
        "    x = start_id\n",
        "    while len(word_ids) < sample_size: # sample_sizeはサンプリングする単語の数\n",
        "      x = np.array(x).reshape(1, 1)\n",
        "      # 各単語のスコアを出力\n",
        "      score = self.predict(x)\n",
        "      # softmax関数で正規化\n",
        "      p = softmax(score.flatten())\n",
        "\n",
        "      # 確率分布からのサンプリング\n",
        "      sampled = np.random.choice(len(p), size=1, p=p)\n",
        "\n",
        "      if (skip_ids is None) or (sampled not in skip_ids): # 不要語などを指定して除外する\n",
        "        x = sampled\n",
        "        word_ids.append(int(x))\n",
        "\n",
        "    return word_ids\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdmoWzUmwFn8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "75659a9d-f299-4aff-d86a-cd0b749293ca"
      },
      "source": [
        "from ch07.rnnlm_gen import RnnlmGen\n",
        "from dataset import ptb\n",
        "\n",
        "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
        "vocab_size = len(word_to_id)\n",
        "corpus_size = len(corpus)\n",
        "\n",
        "model = RnnlmGen()\n",
        "# 学習済みモデルでの実行\n",
        "model.load_params('ch06/Rnnlm.pkl')\n",
        "\n",
        "# start文字とskip文字の設定\n",
        "# start_word = 'the meaning of life is'\n",
        "start_words = 'the meaning of life is'\n",
        "start_ids = [word_to_id[w] for w in start_words.split(' ')]\n",
        "# start_id = word_to_id[start_word]\n",
        "\n",
        "skip_words = ['N', '<unk>', '$']\n",
        "skip_ids = [word_to_id[w] for w in skip_words]\n",
        "\n",
        "# 文章生成\n",
        "for x in start_ids[:-1]:\n",
        "    x = np.array(x).reshape(1, 1)\n",
        "    model.predict(x)\n",
        "\n",
        "word_ids = model.generate(start_ids[-1], skip_ids)\n",
        "word_ids = start_ids[:-1] + word_ids\n",
        "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
        "txt = txt.replace(' <eos>', '.\\n')\n",
        "print('-' * 50)\n",
        "print(txt)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading ptb.train.txt ... \n",
            "Done\n",
            "--------------------------------------------------\n",
            "the meaning of life is price insurance associated with pro-democracy planners without turning to mr. brown.\n",
            " but the alarm approach from these firms included.\n",
            " he adds that progress is not for a congressional leadership before and eidsmo studying cathay 's hard absurd benefits.\n",
            " the latest time of mr. lawson 's bill eliminating many workers strategy will have an capped suit for the cowboys ' plans quickly.\n",
            " but as a matter by the gop statement he said the idea is to be surprised to produce up wells by the poor.\n",
            " mr. roman advised mr. krenz will take over the east\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrcsS4Xnz-wi",
        "colab_type": "text"
      },
      "source": [
        "### さらに良い文章へ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48713mCtxdy_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from ch07.rnnlm_gen import BetterRnnlmGen\n",
        "\n",
        "# corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
        "# vocab_size = len(word_to_id)\n",
        "# corpus_size = len(corpus)\n",
        "\n",
        "# model = BetterRnnlmGen()\n",
        "# # 学習済みモデルでの実行\n",
        "# model.load_params('ch06/BetterRnnlm.pkl')\n",
        "\n",
        "# # start文字とskip文字の設定\n",
        "# start_word = 'you'\n",
        "# start_id = word_to_id[start_word]\n",
        "# skip_words = ['N', '<unk>', '$']\n",
        "# skip_ids = [word_to_id[w] for w in skip_words]\n",
        "\n",
        "# # 文章生成\n",
        "# word_ids = model.generate(start_id, skip_ids)\n",
        "# txt = ' '.join([id_to_word[i] for i in word_ids])\n",
        "# txt = txt.replace(' <eos>', '.\\n')\n",
        "# print(txt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYbRE-g714C-",
        "colab_type": "text"
      },
      "source": [
        "## seq2seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdI1gQDb2IF3",
        "colab_type": "text"
      },
      "source": [
        "### seq2seqの原理\n",
        "+ seq2seqはEncoder-Decoderモデルとも呼ばれる。\n",
        " + エンコードは、情報をある規則に基づいて変換すること\n",
        " + デコードはエンコードされた情報をもとの情報に戻すこと\n",
        " + EncoderはRNNを利用して、時系列データをhという隠れ状態ベクトルに変換\n",
        "  + LSTMの隠れ状態hは固定長のベクトルとなる。よってEncoderは任意の長さの文章を固定長のベクトルに変換するものと考えることができる。\n",
        " + Decoderは隠れ状態ベクトルhをLSTMレイヤで受け取り、固定長のベクトルから任意の長さのベクトルに変換している。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rW4KaiU15XBF",
        "colab_type": "text"
      },
      "source": [
        "### 時系列データの変換用のトイ・プロブレム\n",
        "+ 時系列データの変換問題として、足算をseq2seqで行わせる。\n",
        "+ この問題では文を単語ではなく文字単位で分割することを考える。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugYH3oVp7HxU",
        "colab_type": "text"
      },
      "source": [
        "### 可変長の時系列データ\n",
        "+ 可変長の時系列データをミニバッチ学習するための最も単純な方法はパディングを使うこと。\n",
        " + パディングは本来のデータを無効なデータ（空白文字など）で埋めてデータの長さを均一に揃えるテクニックのことを指す。\n",
        " + パディング専用の処理をseq2seqに加える必要がある。\n",
        "  + Decoderでパディングが入力されたときには損失の結果に計上しないようにする。\n",
        "  + Encoderでパディングが入力されたときには前時刻の入力をそのまま出力するようにする。パディングが存在しなかったかのように扱える。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwPavniox367",
        "colab_type": "text"
      },
      "source": [
        "### 足算データセット"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9xrFoQZ0q-C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "162ecb93-effe-4e2e-92aa-bb21c827dcef"
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "from dataset import sequence\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = sequence.load_data('addition.txt', seed=1984)\n",
        "chart_to_id, id_to_char = sequence.get_vocab()\n",
        "\n",
        "print(x_train.shape, t_train.shape)\n",
        "print(x_test.shape, t_test.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(45000, 7) (45000, 5)\n",
            "(5000, 7) (5000, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGxN9LxJzFUz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "314cb037-3006-4574-a1aa-a57aa2a115b2"
      },
      "source": [
        "print(x_train[0])\n",
        "print(t_train[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 3  0  2  0  0 11  5]\n",
            "[ 6  0 11  7  5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYK32jCezr-k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b99dbb89-49c8-4264-f300-e462c1102363"
      },
      "source": [
        "print(' '.join([id_to_char[c] for c in x_train[0]]))\n",
        "print(' '.join([id_to_char[c] for c in t_train[0]]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7 1 + 1 1 8  \n",
            "_ 1 8 9  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vX01q0ma0fkk",
        "colab_type": "text"
      },
      "source": [
        "## seq2seqの実装"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWNX3qy90nNW",
        "colab_type": "text"
      },
      "source": [
        "### Encoderクラス"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8gGXsNi3CcI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder:\n",
        "  def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "    # 語彙数、文字ベクトルの次元数、LSTMレイヤの隠れ状態ベクトルの次元数\n",
        "    V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "\n",
        "    # 重みパラメータの初期化\n",
        "    rn = np.random.randn\n",
        "    embed_W = (rn(V, D) / 100 ).astype('f')\n",
        "    lstm_Wx = (rn(D, 4*H) / np.sqrt(D)).astype('f')\n",
        "    lstm_Wh = (rn(H, 4*H) / np.sqrt(H)).astype('f')\n",
        "    lstm_b = np.zeros(4*H).astype('f')\n",
        "\n",
        "    self.embed = TimeEmbedding(embed_W)\n",
        "    self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=False) # 今回はTime LSTMレイヤは状態を維持しない\n",
        "\n",
        "    # 重みパラメータと勾配をリストに集約\n",
        "    self.params = self.embed.params + self.lstm.params\n",
        "    self.grads = self.embed.grads + self.lstm.grads\n",
        "    self.hs = None\n",
        "\n",
        "  def forward(self, xs):\n",
        "    xs = self.embed.forward(xs) # time embeddingレイヤ\n",
        "    hs = self.lstm.forward(xs) # time LSTMレイヤ\n",
        "    self.hs = hs\n",
        "    return hs[:, -1, :] # 最後の時刻の隠れ状態だけを取り出す\n",
        "\n",
        "  def backward(self, dh):\n",
        "    dhs = np.zeros_like(self.hs)\n",
        "    dhs[:, -1, :] = dh # LSTMレイヤの最後の隠れ状態に対する勾配\n",
        "\n",
        "    dout = self.lstm.backward(dhs) # time LSTMレイヤ\n",
        "    dout = self.embed.backward(dout) # time embeddingレイヤ\n",
        "    return dout"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZ4E5Hex0po9",
        "colab_type": "text"
      },
      "source": [
        "### Decoderクラス"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPfaQwYM3DPf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder:\n",
        "  def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "    V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "    rn = np.random.randn\n",
        "    embed_W = (rn(V, D) / 100).astype('f')\n",
        "    lstm_Wx = (rn(D, 4*H) / np.sqrt(D)).astype('f')\n",
        "    lstm_Wh = (rn(D, 4*H) / np.sqrt(H)).astype('f')\n",
        "    lstm_b = np.zeros(4*H).astype('f')\n",
        "    affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
        "    affine_b = np.zeros(V).astype('f')\n",
        "\n",
        "    self.embed = TimeEmbedding(embed_W)\n",
        "    self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
        "    self.affine = TimeAffine(affine_W, affine_b)\n",
        "\n",
        "    self.params, self.grads = [], []\n",
        "\n",
        "    for layer in (self.embed, self.lstm, self.affine):\n",
        "      self.params += layer.params\n",
        "      self.grads += layer.grads\n",
        "\n",
        "  def forward(self, xs, h):\n",
        "    self.lstm.set_state(h)\n",
        "\n",
        "    out = self.embed.forward(xs)\n",
        "    out = self.lstm.forward(out)\n",
        "    score = self.affine.forward(out)\n",
        "    return score\n",
        "\n",
        "  def backward(self, dscore):\n",
        "    dout = self.affine.backward(dscore) # softmax with lossレイヤから勾配を受け取って、Time Affineレイヤを通す\n",
        "    dout = self.lstm.backward(dout) # Time LSTMレイヤを通す\n",
        "    dout = self.embed.backward(dout) # Time Embeddingレイヤを通す\n",
        "    dh = self.lstm.dh # 時間方向の勾配を取り出す\n",
        "    return dh\n",
        "\n",
        "  def generate(self, h, start_id, sample_size):\n",
        "    sampled = []\n",
        "    sample_id = start_id # 最初の単語のid\n",
        "    self.lstm.set_state(h) # 最初の隠れ状態\n",
        "\n",
        "    for _ in range(sample_size): # 生成する文字数\n",
        "      x = np.array(sample_id).reshape((1, 1))\n",
        "      out = self.embed.forward(x) # Time Embeddingレイヤ\n",
        "      out = self.lstm.forward(out) # Time LSTMレイヤ\n",
        "      score = self.affine.forward(out) # Tiem Affineレイヤ\n",
        "\n",
        "      sample_id = np.argmax(score.flatten()) # affineレイヤが出力するスコアから最大値を持つ文字idを選ぶ\n",
        "      sampled.append(int(sample_id))\n",
        "\n",
        "    return sampled"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQT66wWQ27Vd",
        "colab_type": "text"
      },
      "source": [
        "### Seq2Seqクラス"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njuTLNLreuMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.time_layers import *\n",
        "from common.base_model import BaseModel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdCoEg5ez41m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2seq(BaseModel):\n",
        "  def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "    V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "    self.encoder = encoder(V, D, H)\n",
        "    self.decoder = Decoder(V, D, H)\n",
        "    self.softmax = TimeSoftmaxWithLoss()\n",
        "\n",
        "    self.params = self.encoder.params + self.decoder.params\n",
        "    self.grads = self.encoder.grads + self.decoder.grads\n",
        "\n",
        "  def forward(self, xs, ts):\n",
        "    decoder_xs, decoder_ts = ts[:, :-1], ts[:, 1:]\n",
        "\n",
        "    h = self.encoder.forward(xs)\n",
        "    score = self.decoder.forward(decoder_xs, h)\n",
        "    loss = self.softmax.forward(score, decoder_ts)\n",
        "    return loss\n",
        "\n",
        "  def backward(self, dout=1):\n",
        "    dout = self.softmax.backward(dout)\n",
        "    dh = self.decoder.backward(dout)\n",
        "    dout = self.encoder.backward(dh)\n",
        "    return dout\n",
        "\n",
        "  def generate(self, xs, start_id, sample_size):\n",
        "    h = self.encoder.forward(xs)\n",
        "    sampled = self.decoder.generate(h, start_id, sample_size)\n",
        "    return sampled"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E_wqL3geCIV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}