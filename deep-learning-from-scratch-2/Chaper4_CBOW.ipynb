{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chaper4_CBOW.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMtzORGLA/zOQi5RA7lzKh6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KamonohashiPerry/MachineLearning/blob/master/deep-learning-from-scratch-2/Chaper4_CBOW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQU4253TTkov",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "41f9058e-7692-4b1b-83a8-28d1158e0292"
      },
      "source": [
        "!git clone https://github.com/oreilly-japan/deep-learning-from-scratch-2.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'deep-learning-from-scratch-2'...\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 378 (delta 0), reused 0 (delta 0), pack-reused 373\u001b[K\n",
            "Receiving objects: 100% (378/378), 7.91 MiB | 5.16 MiB/s, done.\n",
            "Resolving deltas: 100% (210/210), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxN5wRHgUHpQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "75f947ef-cbdf-4415-ba94-c8519b3592fa"
      },
      "source": [
        "cd deep-learning-from-scratch-2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/deep-learning-from-scratch-2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7AAbVN4UQIk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "47c51477-8beb-4cf1-8d40-02dd2ab8b677"
      },
      "source": [
        "ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mch01\u001b[0m/  \u001b[01;34mch03\u001b[0m/  \u001b[01;34mch05\u001b[0m/  \u001b[01;34mch07\u001b[0m/  \u001b[01;34mcommon\u001b[0m/   LICENSE.md\n",
            "\u001b[01;34mch02\u001b[0m/  \u001b[01;34mch04\u001b[0m/  \u001b[01;34mch06\u001b[0m/  \u001b[01;34mch08\u001b[0m/  \u001b[01;34mdataset\u001b[0m/  README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YANXkJ66TwD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append('..')\n",
        "from common import config\n",
        "# GPUで実行する場合は、下記のコメントアウトを消去（要cupy）\n",
        "# ===============================================\n",
        "config.GPU = True\n",
        "# ===============================================\n",
        "from common.np import *\n",
        "import pickle\n",
        "from common.trainer import Trainer\n",
        "from common.optimizer import Adam\n",
        "from ch04.cbow import CBOW\n",
        "from ch04.skip_gram import SkipGram\n",
        "from common.util import create_contexts_target, to_cpu, to_gpu\n",
        "from dataset import ptb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cms2TSZ8UGCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ハイパーパラメータの設定\n",
        "window_size = 5\n",
        "hidden_size = 100\n",
        "batch_size = 100\n",
        "max_epoch = 10\n",
        "\n",
        "# データの読み込み\n",
        "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
        "vocab_size = len(word_to_id)\n",
        "\n",
        "contexts, target = create_contexts_target(corpus, window_size)\n",
        "if config.GPU:\n",
        "    contexts, target = to_gpu(contexts), to_gpu(target)\n",
        "\n",
        "# モデルなどの生成\n",
        "model = CBOW(vocab_size, hidden_size, window_size, corpus)\n",
        "# model = SkipGram(vocab_size, hidden_size, window_size, corpus)\n",
        "optimizer = Adam()\n",
        "trainer = Trainer(model, optimizer)\n",
        "\n",
        "# 学習開始\n",
        "trainer.fit(contexts, target, max_epoch, batch_size)\n",
        "trainer.plot()\n",
        "\n",
        "# 後ほど利用できるように、必要なデータを保存\n",
        "word_vecs = model.word_vecs\n",
        "if config.GPU:\n",
        "    word_vecs = to_cpu(word_vecs)\n",
        "params = {}\n",
        "params['word_vecs'] = word_vecs.astype(np.float16)\n",
        "params['word_to_id'] = word_to_id\n",
        "params['id_to_word'] = id_to_word\n",
        "pkl_file = 'cbow_params.pkl'  # or 'skipgram_params.pkl'\n",
        "with open(pkl_file, 'wb') as f:\n",
        "    pickle.dump(params, f, -1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqB8mmDEU3SE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "from common import config\n",
        "config.GPU = False\n",
        "\n",
        "from common.np import *\n",
        "\n",
        "from common.util import most_similar, analogy\n",
        "import pickle\n",
        "\n",
        "pkl_file = 'cbow_params.pkl'\n",
        "# pkl_file = 'skipgram_params.pkl'\n",
        "\n",
        "with open(pkl_file, 'rb') as f:\n",
        "    params = pickle.load(f)\n",
        "    word_vecs = params['word_vecs']\n",
        "    word_to_id = params['word_to_id']\n",
        "    id_to_word = params['id_to_word']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEPIIAg6fQIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def cos_similarity(x, y, eps=1e-8):\n",
        "    '''コサイン類似度の算出\n",
        "    :param x: ベクトル\n",
        "    :param y: ベクトル\n",
        "    :param eps: ”0割り”防止のための微小値\n",
        "    :return:\n",
        "    '''\n",
        "    nx = x / (np.sqrt(np.sum(x ** 2)) + eps)\n",
        "    ny = y / (np.sqrt(np.sum(y ** 2)) + eps)\n",
        "    return np.dot(nx, ny)\n",
        "\n",
        "\n",
        "def most_similar_v2(query, word_to_id, id_to_word, word_matrix, top=5):\n",
        "    '''類似単語の検索\n",
        "    :param query: クエリ（テキスト）\n",
        "    :param word_to_id: 単語から単語IDへのディクショナリ\n",
        "    :param id_to_word: 単語IDから単語へのディクショナリ\n",
        "    :param word_matrix: 単語ベクトルをまとめた行列。各行に対応する単語のベクトルが格納されていることを想定する\n",
        "    :param top: 上位何位まで表示するか\n",
        "    '''\n",
        "    if query not in word_to_id:\n",
        "        print('%s is not found' % query)\n",
        "        return\n",
        "\n",
        "    print('\\n[query] ' + query)\n",
        "    query_id = word_to_id[query]\n",
        "    query_vec = word_matrix[query_id]\n",
        "\n",
        "    vocab_size = len(id_to_word)\n",
        "\n",
        "    similarity = np.zeros(vocab_size)\n",
        "    for i in range(vocab_size):\n",
        "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
        "\n",
        "    count = 0\n",
        "    for i in (-1 * similarity).argsort():\n",
        "        if id_to_word[i] == query:\n",
        "            continue\n",
        "        print(' %s: %s' % (id_to_word[i], similarity[i]))\n",
        "\n",
        "        count += 1\n",
        "        if count >= top:\n",
        "            return\n",
        "\n",
        "def analogy(a, b, c, word_to_id, id_to_word, word_matrix, top=5, answer=None):\n",
        "    for word in (a, b, c):\n",
        "        if word not in word_to_id:\n",
        "            print('%s is not found' % word)\n",
        "            return\n",
        "\n",
        "    print('\\n[analogy] ' + a + ':' + b + ' = ' + c + ':?')\n",
        "    a_vec, b_vec, c_vec = word_matrix[word_to_id[a]], word_matrix[word_to_id[b]], word_matrix[word_to_id[c]]\n",
        "    query_vec = b_vec - a_vec + c_vec\n",
        "    query_vec = normalize(query_vec)\n",
        "\n",
        "    similarity = np.dot(word_matrix, query_vec)\n",
        "\n",
        "    if answer is not None:\n",
        "        print(\"==>\" + answer + \":\" + str(np.dot(word_matrix[word_to_id[answer]], query_vec)))\n",
        "\n",
        "    count = 0\n",
        "    for i in (-1 * similarity).argsort():\n",
        "        if np.isnan(similarity[i]):\n",
        "            continue\n",
        "        if id_to_word[i] in (a, b, c):\n",
        "            continue\n",
        "        print(' {0}: {1}'.format(id_to_word[i], similarity[i]))\n",
        "\n",
        "        count += 1\n",
        "        if count >= top:\n",
        "            return\n",
        "\n",
        "def normalize(x):\n",
        "    if x.ndim == 2:\n",
        "        s = np.sqrt((x * x).sum(1))\n",
        "        x /= s.reshape((s.shape[0], 1))\n",
        "    elif x.ndim == 1:\n",
        "        s = np.sqrt((x * x).sum())\n",
        "        x /= s\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dT4gLhJXZyQM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986
        },
        "outputId": "1dfdb4cd-46ee-4185-922c-db106364eed4"
      },
      "source": [
        "# most similar task\n",
        "querys = ['you', 'year', 'car', 'toyota']\n",
        "for query in querys:\n",
        "    most_similar_v2(query, word_to_id, id_to_word, word_vecs, top=5)\n",
        "\n",
        "# analogy task\n",
        "print('-'*50)\n",
        "analogy('king', 'man', 'queen',  word_to_id, id_to_word, word_vecs)\n",
        "analogy('take', 'took', 'go',  word_to_id, id_to_word, word_vecs)\n",
        "analogy('car', 'cars', 'child',  word_to_id, id_to_word, word_vecs)\n",
        "analogy('good', 'better', 'bad',  word_to_id, id_to_word, word_vecs)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[query] you\n",
            " we: 0.720703125\n",
            " i: 0.69482421875\n",
            " they: 0.60595703125\n",
            " anybody: 0.59326171875\n",
            " your: 0.5927734375\n",
            "\n",
            "[query] year\n",
            " month: 0.86376953125\n",
            " summer: 0.78857421875\n",
            " week: 0.77880859375\n",
            " spring: 0.771484375\n",
            " decade: 0.65625\n",
            "\n",
            "[query] car\n",
            " luxury: 0.6240234375\n",
            " truck: 0.615234375\n",
            " auto: 0.61328125\n",
            " window: 0.6083984375\n",
            " cars: 0.578125\n",
            "\n",
            "[query] toyota\n",
            " engines: 0.65087890625\n",
            " mazda: 0.6318359375\n",
            " seita: 0.62841796875\n",
            " honda: 0.62353515625\n",
            " minicomputers: 0.61474609375\n",
            "--------------------------------------------------\n",
            "\n",
            "[analogy] king:man = queen:?\n",
            " a.m: 6.90234375\n",
            " woman: 5.42578125\n",
            " naczelnik: 4.91796875\n",
            " toxin: 4.77734375\n",
            " daffynition: 4.65625\n",
            "\n",
            "[analogy] take:took = go:?\n",
            " went: 4.53125\n",
            " eurodollars: 4.48828125\n",
            " 're: 4.2265625\n",
            " came: 4.16796875\n",
            " were: 4.0625\n",
            "\n",
            "[analogy] car:cars = child:?\n",
            " a.m: 6.859375\n",
            " children: 5.5\n",
            " rape: 5.2109375\n",
            " adults: 4.98046875\n",
            " women: 4.74609375\n",
            "\n",
            "[analogy] good:better = bad:?\n",
            " rather: 5.75390625\n",
            " more: 5.3359375\n",
            " less: 5.25390625\n",
            " greater: 4.328125\n",
            " faster: 3.75390625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiHhuqN4b4aq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}