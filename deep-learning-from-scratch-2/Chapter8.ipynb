{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter8.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM9TQWWj53xA8V3izV7LD8a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KamonohashiPerry/MachineLearning/blob/master/deep-learning-from-scratch-2/Chapter8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cFmfdrBEzxf",
        "colab_type": "text"
      },
      "source": [
        "# Attention\n",
        "+ 近年の深層学習の重要テクニックの一つ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JlDgycxFZ0h",
        "colab_type": "text"
      },
      "source": [
        "## Attentionの仕組み\n",
        "+ 注意機構(Attention Mechanism)\n",
        " + 必要な情報だけに注意を向けさせることができる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYWVE_lUGKCX",
        "colab_type": "text"
      },
      "source": [
        "### seq2seqの問題点\n",
        "+ Encoderを使って固定長に変換しなければならないところ。\n",
        " + 固定長だと限界が低い。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ympohyx9HA6h",
        "colab_type": "text"
      },
      "source": [
        "### Encoderの改良\n",
        "+ Encoderの出力は、入力される文章の長さに応じて、その長さを変えるべき。\n",
        "+ 各時刻のLSTMレイヤの隠れ状態ベクトルを全て利用する。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IqDfpurbPsI",
        "colab_type": "text"
      },
      "source": [
        "### Decoderの改良1\n",
        "+ Encoderは各単語に対応するLSTMレイヤの隠れ状態ベクトルをhsとして出力\n",
        " + このhsがDecoderに渡されて時系列変換が行われる\n",
        "   + これまでのDecoderはEncoderのLSTMレイヤの最後にある隠れ状態だけを利用していた。\n",
        "\n",
        "+ 入力と出力でどの単語が関連しているのかという対応関係をseq2seqに学習させることはできないか。\n",
        " + 必要な情報にだけ注意を向けさせ、その情報から時系列変換を行うことを目指す。\n",
        "   + この点においてもPeekyとは違う。\n",
        "\n",
        "+ 新たに何らかの計算を行うレイヤを追加する\n",
        " + 何らかの計算\n",
        "   + 各時刻においてLSTMレイヤの隠れ状態とEncoderからのhsを受け取る\n",
        "     + 必要な情報だけを選び出す\n",
        "       + それをその先のAffineレイヤへと出力\n",
        "\n",
        "\n",
        "+ 単語のアラインメント抽出\n",
        " + 各時刻においてDecoderへの入力単語と対応関係にある単語のベクトルをhsから選び出す\n",
        "   + ただし、選び出すという操作に関して微分ができない\n",
        "     + ひとつを選ぶのではなく、全てを選ぶというアプローチ\n",
        "       + 各単語の重要度を表す重みを別途計算する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMQKpYrFETXA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "34951287-cf9d-4060-a801-42f6489d8807"
      },
      "source": [
        "!git clone https://github.com/oreilly-japan/deep-learning-from-scratch-2.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'deep-learning-from-scratch-2'...\n",
            "remote: Enumerating objects: 13, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 386 (delta 5), reused 10 (delta 5), pack-reused 373\u001b[K\n",
            "Receiving objects: 100% (386/386), 7.91 MiB | 8.44 MiB/s, done.\n",
            "Resolving deltas: 100% (215/215), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDQqP0AohpVF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "003ffd0e-b792-45c3-8086-7f9e162a866a"
      },
      "source": [
        "cd deep-learning-from-scratch-2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/deep-learning-from-scratch-2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iswkSARhsX5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f25e2b37-1f04-4c7f-ef38-f75673d27141"
      },
      "source": [
        "ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mch01\u001b[0m/  \u001b[01;34mch03\u001b[0m/  \u001b[01;34mch05\u001b[0m/  \u001b[01;34mch07\u001b[0m/  \u001b[01;34mcommon\u001b[0m/   LICENSE.md\n",
            "\u001b[01;34mch02\u001b[0m/  \u001b[01;34mch04\u001b[0m/  \u001b[01;34mch06\u001b[0m/  \u001b[01;34mch08\u001b[0m/  \u001b[01;34mdataset\u001b[0m/  README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DC7uS6G1hstA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b58168fe-a969-436b-b9d9-818f3ca4c28f"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 時系列の長さ、隠れ状態ベクトルの要素数\n",
        "T, H = 5, 4\n",
        "hs = np.random.randn(T, H)\n",
        "a = np.array([0.8, 0.1, 0.03, 0.05, 0.02])\n",
        "\n",
        "# repeatで多次元配列を生成\n",
        "ar = a.reshape(5, 1).repeat(4, axis=1)\n",
        "\n",
        "print(hs.shape)\n",
        "print(ar.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5, 4)\n",
            "(5, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMH8wwn1iA43",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "fe2be6aa-b805-46f8-8fff-f4c90aff2b9b"
      },
      "source": [
        "hs"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.14687232,  1.7449836 ,  0.2707513 ,  1.70613449],\n",
              "       [-1.25281108, -0.23661168,  1.30227449,  1.04917752],\n",
              "       [ 0.86950236,  0.00351564, -0.82740428,  1.08915518],\n",
              "       [-0.21672951, -0.07013253,  0.55358795, -0.03459616],\n",
              "       [-0.25104398,  2.50063861,  0.42609329, -0.24681245]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHg-7GWZiMs5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "dd8fea19-76eb-402a-8bc5-ee6a30d0e34e"
      },
      "source": [
        "ar"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.8 , 0.8 , 0.8 , 0.8 ],\n",
              "       [0.1 , 0.1 , 0.1 , 0.1 ],\n",
              "       [0.03, 0.03, 0.03, 0.03],\n",
              "       [0.05, 0.05, 0.05, 0.05],\n",
              "       [0.02, 0.02, 0.02, 0.02]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV9N5UwMiNHs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 重み付け\n",
        "t = hs * ar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrGoXcgWiV_1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a7f7e4df-cfed-4442-99b8-2002c69570aa"
      },
      "source": [
        "print(t.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLmmNG3YiY2q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a163b855-061f-4ae9-8e5b-6d8d992d5a79"
      },
      "source": [
        "# その重み付けしたものの和\n",
        "c = np.sum(t, axis=0)\n",
        "print(c.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWb4K0qfidd7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "92abf672-0b93-49b1-bdad-d5fba3879415"
      },
      "source": [
        "c"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.00244446, 1.41893733, 0.35820762, 1.49583394])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-Wk0ZHkie6g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "30c8afe1-8aeb-431b-ee65-3b4ca2ec1cae"
      },
      "source": [
        "# バッチ処理版の重み付き和の実装\n",
        "\n",
        "# バッチの数、時系列の長さ、隠れ状態ベクトルの要素数\n",
        "N, T, H = 10, 5, 4\n",
        "hs = np.random.randn(N, T, H)\n",
        "a = np.random.randn(N, T)\n",
        "ar = a.reshape(N, T, 1).repeat(H, axis=2)\n",
        "\n",
        "t = hs * ar\n",
        "print(t.shape)\n",
        "\n",
        "# バッチごとの隠れ状態のベクトルの要素数ごとの重み付けの合計値\n",
        "c = np.sum(t, axis=1)\n",
        "print(c.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 5, 4)\n",
            "(10, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-r32FxJOn6PJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# コンテキストベクトルをもとめるWeightSumレイヤ\n",
        "class WeightSum:\n",
        "  def __init__(self):\n",
        "    self.params, self.grads = [], []\n",
        "    self.cache = None\n",
        "\n",
        "  def forward(self, hs, a):\n",
        "    N, T, H = hs.shape\n",
        "\n",
        "    ar = a.reshape(N, T, 1).repeat(H, axis=2)\n",
        "    t = hs*ar\n",
        "    c = np.sum(t, axis=1)\n",
        "\n",
        "    self.cache = (hs, ar)\n",
        "    return c\n",
        "\n",
        "  def backward(self, dc):\n",
        "    hs, ar = self.cache\n",
        "    N, T, H = hs.shape\n",
        "\n",
        "    dt = dc.reshape(N, 1, H).repeat(T, axis=1) # sumの逆伝播\n",
        "    dar = dt*hs\n",
        "    dhs = dt*ar\n",
        "    da = np.sum(dar, axis=2) # repeatの逆伝播\n",
        "\n",
        "    return dhs, da"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzQK5enrpqnz",
        "colab_type": "text"
      },
      "source": [
        "### Decoderの改良2\n",
        "+ 各単語の重要度を表す重みaを計算したい。それをもとにコンテキストベクトルを計算できる。\n",
        " + DecoderのLSTMレイヤの隠れ状態ベクトルhが、Encoderの出力であるhsの各単語ベクトルとどれだけ似ているかを数値で表すこと。\n",
        "   + 最も簡単なのはベクトルの内積を利用するもの。\n",
        "\n",
        "$$a \\cdot\tb = a_1b_1 + a_2b_2 + \\dots + a_nb_n $$\n",
        "\n",
        "\n",
        "+ 類似度はsoftmax関数で正規化する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6xnniAfpebJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "86102fd5-54ac-4420-b7fc-85b4bafe163e"
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.layers import Softmax\n",
        "import numpy as np\n",
        "\n",
        "N, T, H = 10, 5, 4\n",
        "hs = np.random.randn(N, T, H)\n",
        "h = np.random.randn(N, H)\n",
        "hr = h.reshape(N, 1, H).repeat(T, axis=1)\n",
        "\n",
        "t = hs*hr\n",
        "print(t.shape)\n",
        "\n",
        "s = np.sum(t, axis=2)\n",
        "print(s.shape)\n",
        "\n",
        "softmax = Softmax()\n",
        "a = softmax.forward(s)\n",
        "print(a.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 5, 4)\n",
            "(10, 5)\n",
            "(10, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHqiOztlW2Z6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.np import *\n",
        "from common.layers import Softmax\n",
        "\n",
        "class AttentionWeight:\n",
        "  def __init__(self):\n",
        "    self.params, self.grads = [], []\n",
        "    self.softmax = Softmax()\n",
        "    self.cache = None\n",
        "\n",
        "  def forward(self, hs, h):\n",
        "    N, T, H = hs.shape\n",
        "\n",
        "    hr = h.reshape(N, 1, H).repeat(T, axis=1)\n",
        "    t = hs*hr\n",
        "    s = np.sum(t, axis=2)\n",
        "    a = self.softmax.forward(s)\n",
        "\n",
        "    self.cache = (hs, hr)\n",
        "    return a\n",
        "\n",
        "  def backward(self, da):\n",
        "    hs, hr = self.cache\n",
        "    N, T, H = hs.shape\n",
        "\n",
        "    ds = self.softmax.backward(da)\n",
        "    dhs = dt*hr\n",
        "    dhr = dt*hs\n",
        "    dh = np.sum(dhr, axis=1)\n",
        "\n",
        "    return dhs, dh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JR504nb2adIc",
        "colab_type": "text"
      },
      "source": [
        "### Decoderの改良3\n",
        "+ Attention WeightレイヤとWeight Sumレイヤを組み合わせることでコンテキストベクトルを求めれるようになる。\n",
        " + Attention WeightレイヤがEncoderが出力する各単語のベクトルhsに対して注意をはらい、各単語の重みaを計算する。\n",
        "  + Weight Sumレイヤがaとhsの重み付き和を求めて、コンテキストベクトルcとして出力する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mqy_YRYOYhvE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention:\n",
        "  def __init__(self):\n",
        "    self.params, self.grads = [], []\n",
        "    self.attention_weight_layer = AttentionWeight()\n",
        "    self.weight_sum_layer = WeightSum()\n",
        "    self.attention_weight = None\n",
        "\n",
        "  def forward(self, hs, h):\n",
        "    a = self.attention_weight_layer.forward(hs, h)\n",
        "    out = self.weight_sum_layer.forward(hs, a)\n",
        "    self.attention_weight = a\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dhs0, da = self.weight_sum_layer.backward(dout)\n",
        "    dhs1, dh = self.attention_weight_layer.backward(da)\n",
        "    dhs = dhs0 + dhs1\n",
        "    return dhs, dh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDUD6M-TezI1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TimeAttention:\n",
        "  def __init__(self):\n",
        "    self.params, self.grads = [], []\n",
        "    self.layers = None\n",
        "    self.attention_weights = None # 各Attentionレイヤの各単語への重み\n",
        "\n",
        "  # 順伝播\n",
        "  def forward(self, hs_enc, hs_dec):\n",
        "    # T個分のAttentionレイヤを作成する\n",
        "    N, T, H = hs_dec.shape\n",
        "    out = np.empty_like(hs_dec)\n",
        "    self.layers = []\n",
        "    self.attention_weights = []\n",
        "\n",
        "    for t in range(T):\n",
        "      layer = Attention()\n",
        "      out[:, t, :] = layer.forward(hs_enc, hs_dec[:, t, :])\n",
        "      self.layers.append(layer)\n",
        "      self.attention_weights.append(layer.attention_weight)\n",
        "\n",
        "    return out\n",
        "\n",
        "  # 逆伝播\n",
        "  def backward(self, dout):\n",
        "    # T個分のAttentionレイヤを作成する\n",
        "    N, T, H = dout.shape\n",
        "    dhs_enc = 0\n",
        "    dhs_dec = np.empty_like(dout)\n",
        "\n",
        "    for t in range(T):\n",
        "      layer = self.layers[t]\n",
        "      dhs, dh = layer.backward(dout[:, t, :])\n",
        "      dhs_enc += dhs\n",
        "      dhs_dec[:, t, :] = dh\n",
        "\n",
        "    return dhs_enc, dhs_dec\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIVdolZji85u",
        "colab_type": "text"
      },
      "source": [
        "## Attention付きのseq2seqの実装"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaVCKpCCjajp",
        "colab_type": "text"
      },
      "source": [
        "### Encoderの実装"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-YsVNn6g_dr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.time_layers import *\n",
        "from ch07.seq2seq import Encoder, Seq2seq\n",
        "from ch08.attention_layer import TimeAttention\n",
        "\n",
        "class AttentionEncoder(Encoder):\n",
        "  def forward(self, xs):\n",
        "    xs = self.embed.forward(xs)\n",
        "    hs = self.lstm.forward(xs)\n",
        "    return hs\n",
        "\n",
        "  def backward(self, dhs):\n",
        "    dout = self.lstm.backward(dhs)\n",
        "    dout = self.embed.backward(dout)\n",
        "    return dout"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxpWYr7-kk47",
        "colab_type": "text"
      },
      "source": [
        "### Decoderの実装"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sqmq-vw-kTzN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttentionDecoder:\n",
        "  def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "    V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "    rn = np.random.randn\n",
        "\n",
        "    embed_W = (rn(V, D) / 100).astype('f')\n",
        "    lstm_Wx = (rn(D, 4*H) / np.sqrt(D) ).astype('f')\n",
        "    lstm_Wh = (rn(H, 4*H) / np.sqrt(H) ).astype('f')\n",
        "    lstm_b = np.zeros(4*H).astype('f')\n",
        "    affine_W = (rn(2*H, V) / np.sqrt(2*H)).astype('f')\n",
        "    affine_b = np.zeros(V).astype('f')\n",
        "\n",
        "    self.embed = TimeEmbedding(embed_W)\n",
        "    self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
        "    self.attention = TimeAttention()\n",
        "    self.affine = TimeAffine(affine_W, affine_b)\n",
        "    layers = [self.embed, self.lstm, self.attention, self.affine]\n",
        "\n",
        "    self.params, self.grads = [], []\n",
        "    for layer in layers:\n",
        "      self.params += layer.params\n",
        "      self.grads += layer.grads\n",
        "\n",
        "  def forward(self, xs, enc_hs):\n",
        "    h = enc_hs[:, -1]\n",
        "    self.lstm.set_state(h)\n",
        "\n",
        "    out = self.embed.forward(out)\n",
        "    # LSTMレイヤ\n",
        "    dec_hs = self.lstm.forward(out)\n",
        "    # TimeAttentionレイヤ\n",
        "    c = self.attention.forward(enc_hs, dec_hs)\n",
        "\n",
        "    # TimeAttentionレイヤとLSTMレイヤの出力を結合\n",
        "    out = np.concatenate((c, dec_hs), axis=2)\n",
        "    score = self.affine.forward(out)\n",
        "\n",
        "    return score\n",
        "\n",
        "  def backward(self, dscore):\n",
        "    dout = self.affine.backward(dscore)\n",
        "    N, T, H2 = dout.shape\n",
        "    H = H2 // 2\n",
        "\n",
        "    dc, ddec_hs0 = dout[:, :, :H], dout[:, :, H:]\n",
        "    denc_hs, ddec_hs1 = self.attention.backward(dc)\n",
        "    ddec_hs = ddec_hs0 + ddec_hs1\n",
        "    dout = self.lstm.dh\n",
        "    denc_hs[:, -1] += dh\n",
        "    self.embed.backward(dout)\n",
        "\n",
        "    return denc_hs\n",
        "\n",
        "  def generate(self, emc_hs, start_id, sample_size):\n",
        "    sampled = []\n",
        "    sample_id = start_id\n",
        "    h = enc_hs[:, -1]\n",
        "    self.lstm.set_state(h)\n",
        "\n",
        "    for _ in range(sample_size):\n",
        "      x = np.array([sample_id]).reshape((1, 1))\n",
        "\n",
        "      out = self.embed.forward(x)\n",
        "      dec_hs = self.lstm.forward(out)\n",
        "      c = self.attention.forward(enc_hs, dec_hs)\n",
        "      out = np.concatenate((c, dec_hs), axis=2)\n",
        "      score = sel.affine.forward(out)\n",
        "\n",
        "      sample_id = np.argmax(score.flatten())\n",
        "      sampled.append(sample_id)\n",
        "    return sampled\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJefVyshrZJZ",
        "colab_type": "text"
      },
      "source": [
        "### seq2seqの実装"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lIXdqjVqHA5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from ch07.seq2seq import Encoder, Seq2seq\n",
        "\n",
        "class AttentionSeq2seq(Seq2seq):\n",
        "  def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "    args = vocab_size, wordvec_size ,hidden_size\n",
        "    self.encoder = AttentionEncoder(*args)\n",
        "    self.decoder = AttentionDecoder(*args)\n",
        "    self.softmax = TimeSoftmaxWithLoss()\n",
        "\n",
        "    self.params = self.encoder.params + self.decoder.params\n",
        "    self.grads = self.encoder.grads + self.decoder.grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvOpA3CLsumP",
        "colab_type": "text"
      },
      "source": [
        "## Attentionの評価\n",
        "+ 日付フォーマットを変換する問題を扱う"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Suky7DY9tBiC",
        "colab_type": "text"
      },
      "source": [
        "### 日付フォーマットの変換問題"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqqW8IgWtj_z",
        "colab_type": "text"
      },
      "source": [
        "### Attention付きseq2seqの学習"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UG7PbMqx5yd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.time_layers import *\n",
        "from ch07.seq2seq import Seq2seq, Encoder\n",
        "\n",
        "\n",
        "class PeekyDecoder:\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        embed_W = (rn(V, D) / 100).astype('f')\n",
        "        lstm_Wx = (rn(H + D, 4 * H) / np.sqrt(H + D)).astype('f')\n",
        "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b = np.zeros(4 * H).astype('f')\n",
        "        affine_W = (rn(H + H, V) / np.sqrt(H + H)).astype('f')\n",
        "        affine_b = np.zeros(V).astype('f')\n",
        "\n",
        "        self.embed = TimeEmbedding(embed_W)\n",
        "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
        "        self.affine = TimeAffine(affine_W, affine_b)\n",
        "\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in (self.embed, self.lstm, self.affine):\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, xs, h):\n",
        "        N, T = xs.shape\n",
        "        N, H = h.shape\n",
        "\n",
        "        self.lstm.set_state(h)\n",
        "\n",
        "        out = self.embed.forward(xs)\n",
        "        hs = np.repeat(h, T, axis=0).reshape(N, T, H)\n",
        "        out = np.concatenate((hs, out), axis=2)\n",
        "\n",
        "        out = self.lstm.forward(out)\n",
        "        out = np.concatenate((hs, out), axis=2)\n",
        "\n",
        "        score = self.affine.forward(out)\n",
        "        self.cache = H\n",
        "        return score\n",
        "\n",
        "    def backward(self, dscore):\n",
        "        H = self.cache\n",
        "\n",
        "        dout = self.affine.backward(dscore)\n",
        "        dout, dhs0 = dout[:, :, H:], dout[:, :, :H]\n",
        "        dout = self.lstm.backward(dout)\n",
        "        dembed, dhs1 = dout[:, :, H:], dout[:, :, :H]\n",
        "        self.embed.backward(dembed)\n",
        "\n",
        "        dhs = dhs0 + dhs1\n",
        "        dh = self.lstm.dh + np.sum(dhs, axis=1)\n",
        "        return dh\n",
        "\n",
        "    def generate(self, h, start_id, sample_size):\n",
        "        sampled = []\n",
        "        char_id = start_id\n",
        "        self.lstm.set_state(h)\n",
        "\n",
        "        H = h.shape[1]\n",
        "        peeky_h = h.reshape(1, 1, H)\n",
        "        for _ in range(sample_size):\n",
        "            x = np.array([char_id]).reshape((1, 1))\n",
        "            out = self.embed.forward(x)\n",
        "\n",
        "            out = np.concatenate((peeky_h, out), axis=2)\n",
        "            out = self.lstm.forward(out)\n",
        "            out = np.concatenate((peeky_h, out), axis=2)\n",
        "            score = self.affine.forward(out)\n",
        "\n",
        "            char_id = np.argmax(score.flatten())\n",
        "            sampled.append(char_id)\n",
        "\n",
        "        return sampled\n",
        "\n",
        "\n",
        "class PeekySeq2seq(Seq2seq):\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        self.encoder = Encoder(V, D, H)\n",
        "        self.decoder = PeekyDecoder(V, D, H)\n",
        "        self.softmax = TimeSoftmaxWithLoss()\n",
        "\n",
        "        self.params = self.encoder.params + self.decoder.params\n",
        "        self.grads = self.encoder.grads + self.decoder.grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sb5MYr7RsoPx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6ea67884-cb16-4a9d-d3e7-2bbe0388bca9"
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "import numpy as np\n",
        "from dataset import sequence\n",
        "from common.optimizer import Adam\n",
        "from common.trainer import Trainer\n",
        "from common.util import eval_seq2seq\n",
        "from ch08.attention_seq2seq import AttentionSeq2seq\n",
        "# from ch07.seq2seq import Seq2seq\n",
        "# from ch07.peeky_seq2seq import PeekySeq2seq\n",
        "\n",
        "# データの読み込み\n",
        "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
        "char_to_id, id_to_char = sequence.get_vocab()\n",
        "\n",
        "# 入力文を反転\n",
        "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
        "\n",
        "# ハイパーパラメータの設定\n",
        "vocab_size = len(char_to_id)\n",
        "wordvec_size = 16\n",
        "hidden_size = 16\n",
        "batch_size = 128\n",
        "max_epoch = 10\n",
        "max_grad = 5.0\n",
        "\n",
        "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
        "optimizer = Adam()\n",
        "trainer = Trainer(model, optimizer)\n",
        "\n",
        "acc_list = []\n",
        "for epoch in range(max_epoch):\n",
        "  trainer.fit(x_train, t_train, max_epoch=1,\n",
        "              batch_size=batch_size, max_grad=max_grad)\n",
        "  \n",
        "  correct_num = 0\n",
        "  for i in range(len(x_test)):\n",
        "    question, correct = x_test[[i]], t_test[[i]]\n",
        "    verbose = i < 10\n",
        "    correct_num += eval_seq2seq(model, question, correct,\n",
        "                                    id_to_char, verbose, is_reverse=True)\n",
        "  \n",
        "  acc = float(correct_num) / len(x_test)\n",
        "  acc_list.append(acc)\n",
        "  print('val acc %.3f%%' % (acc * 100))\n",
        "\n",
        "model.save_params()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| epoch 1 |  iter 1 / 351 | time 0[s] | loss 4.08\n",
            "| epoch 1 |  iter 21 / 351 | time 1[s] | loss 4.03\n",
            "| epoch 1 |  iter 41 / 351 | time 2[s] | loss 3.80\n",
            "| epoch 1 |  iter 61 / 351 | time 4[s] | loss 3.06\n",
            "| epoch 1 |  iter 81 / 351 | time 5[s] | loss 2.41\n",
            "| epoch 1 |  iter 101 / 351 | time 6[s] | loss 2.19\n",
            "| epoch 1 |  iter 121 / 351 | time 8[s] | loss 2.12\n",
            "| epoch 1 |  iter 141 / 351 | time 9[s] | loss 2.07\n",
            "| epoch 1 |  iter 161 / 351 | time 11[s] | loss 2.02\n",
            "| epoch 1 |  iter 181 / 351 | time 12[s] | loss 1.97\n",
            "| epoch 1 |  iter 201 / 351 | time 14[s] | loss 1.92\n",
            "| epoch 1 |  iter 221 / 351 | time 15[s] | loss 1.87\n",
            "| epoch 1 |  iter 241 / 351 | time 16[s] | loss 1.81\n",
            "| epoch 1 |  iter 261 / 351 | time 18[s] | loss 1.75\n",
            "| epoch 1 |  iter 281 / 351 | time 19[s] | loss 1.69\n",
            "| epoch 1 |  iter 301 / 351 | time 21[s] | loss 1.63\n",
            "| epoch 1 |  iter 321 / 351 | time 22[s] | loss 1.58\n",
            "| epoch 1 |  iter 341 / 351 | time 24[s] | loss 1.53\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[91m☒\u001b[0m 199-0-0-0-\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[91m☒\u001b[0m 199-0-0-0-\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[91m☒\u001b[0m 199-0-0-0-\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[91m☒\u001b[0m 199-0-0-0-\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[91m☒\u001b[0m 199-0-0-0-\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[91m☒\u001b[0m 199-0-0-0-\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[91m☒\u001b[0m 199-0-0-0-\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[91m☒\u001b[0m 199-0-0-0-\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[91m☒\u001b[0m 199-0-0-0-\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[91m☒\u001b[0m 199-0-0-0-\n",
            "---\n",
            "val acc 0.000%\n",
            "| epoch 2 |  iter 1 / 351 | time 0[s] | loss 1.47\n",
            "| epoch 2 |  iter 21 / 351 | time 1[s] | loss 1.45\n",
            "| epoch 2 |  iter 41 / 351 | time 2[s] | loss 1.40\n",
            "| epoch 2 |  iter 61 / 351 | time 4[s] | loss 1.36\n",
            "| epoch 2 |  iter 81 / 351 | time 5[s] | loss 1.32\n",
            "| epoch 2 |  iter 101 / 351 | time 7[s] | loss 1.29\n",
            "| epoch 2 |  iter 121 / 351 | time 8[s] | loss 1.26\n",
            "| epoch 2 |  iter 141 / 351 | time 9[s] | loss 1.23\n",
            "| epoch 2 |  iter 161 / 351 | time 11[s] | loss 1.21\n",
            "| epoch 2 |  iter 181 / 351 | time 12[s] | loss 1.19\n",
            "| epoch 2 |  iter 201 / 351 | time 14[s] | loss 1.17\n",
            "| epoch 2 |  iter 221 / 351 | time 15[s] | loss 1.15\n",
            "| epoch 2 |  iter 241 / 351 | time 17[s] | loss 1.14\n",
            "| epoch 2 |  iter 261 / 351 | time 18[s] | loss 1.13\n",
            "| epoch 2 |  iter 281 / 351 | time 19[s] | loss 1.12\n",
            "| epoch 2 |  iter 301 / 351 | time 21[s] | loss 1.11\n",
            "| epoch 2 |  iter 321 / 351 | time 22[s] | loss 1.10\n",
            "| epoch 2 |  iter 341 / 351 | time 24[s] | loss 1.09\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[91m☒\u001b[0m 1997-05-05\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[91m☒\u001b[0m 1997-05-05\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[91m☒\u001b[0m 2000-06-06\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[91m☒\u001b[0m 2000-06-06\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[91m☒\u001b[0m 1997-05-05\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[91m☒\u001b[0m 1997-05-05\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[91m☒\u001b[0m 2001-06-06\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[91m☒\u001b[0m 2001-06-06\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[91m☒\u001b[0m 1997-06-05\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[91m☒\u001b[0m 2001-06-06\n",
            "---\n",
            "val acc 0.020%\n",
            "| epoch 3 |  iter 1 / 351 | time 0[s] | loss 1.10\n",
            "| epoch 3 |  iter 21 / 351 | time 1[s] | loss 1.08\n",
            "| epoch 3 |  iter 41 / 351 | time 2[s] | loss 1.08\n",
            "| epoch 3 |  iter 61 / 351 | time 4[s] | loss 1.07\n",
            "| epoch 3 |  iter 81 / 351 | time 5[s] | loss 1.06\n",
            "| epoch 3 |  iter 101 / 351 | time 7[s] | loss 1.05\n",
            "| epoch 3 |  iter 121 / 351 | time 8[s] | loss 1.05\n",
            "| epoch 3 |  iter 141 / 351 | time 10[s] | loss 1.04\n",
            "| epoch 3 |  iter 161 / 351 | time 11[s] | loss 1.04\n",
            "| epoch 3 |  iter 181 / 351 | time 12[s] | loss 1.03\n",
            "| epoch 3 |  iter 201 / 351 | time 14[s] | loss 1.03\n",
            "| epoch 3 |  iter 221 / 351 | time 15[s] | loss 1.02\n",
            "| epoch 3 |  iter 241 / 351 | time 17[s] | loss 1.02\n",
            "| epoch 3 |  iter 261 / 351 | time 18[s] | loss 1.01\n",
            "| epoch 3 |  iter 281 / 351 | time 20[s] | loss 1.01\n",
            "| epoch 3 |  iter 301 / 351 | time 21[s] | loss 1.01\n",
            "| epoch 3 |  iter 321 / 351 | time 22[s] | loss 1.00\n",
            "| epoch 3 |  iter 341 / 351 | time 24[s] | loss 1.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[91m☒\u001b[0m 1988-08-13\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[91m☒\u001b[0m 1997-02-12\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[91m☒\u001b[0m 2001-02-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[91m☒\u001b[0m 2001-02-25\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[91m☒\u001b[0m 1997-04-13\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[91m☒\u001b[0m 1987-08-13\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[91m☒\u001b[0m 1987-02-12\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[91m☒\u001b[0m 1987-02-12\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[91m☒\u001b[0m 1987-08-12\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[91m☒\u001b[0m 1997-08-13\n",
            "---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fks3_l5xXLV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# グラフの描画\n",
        "x = np.arange(len(acc_list))\n",
        "plt.plot(x, acc_list, marker='o')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.ylim(-0.05, 1.05)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68LKx-7gyAgq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}