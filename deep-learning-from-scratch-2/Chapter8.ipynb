{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter8.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN3xpAwFPlyLa86aQTWlBwt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KamonohashiPerry/MachineLearning/blob/master/deep-learning-from-scratch-2/Chapter8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cFmfdrBEzxf",
        "colab_type": "text"
      },
      "source": [
        "# Attention\n",
        "+ 近年の深層学習の重要テクニックの一つ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JlDgycxFZ0h",
        "colab_type": "text"
      },
      "source": [
        "## Attentionの仕組み\n",
        "+ 注意機構(Attention Mechanism)\n",
        " + 必要な情報だけに注意を向けさせることができる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYWVE_lUGKCX",
        "colab_type": "text"
      },
      "source": [
        "### seq2seqの問題点\n",
        "+ Encoderを使って固定長に変換しなければならないところ。\n",
        " + 固定長だと限界が低い。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ympohyx9HA6h",
        "colab_type": "text"
      },
      "source": [
        "### Encoderの改良\n",
        "+ Encoderの出力は、入力される文章の長さに応じて、その長さを変えるべき。\n",
        "+ 各時刻のLSTMレイヤの隠れ状態ベクトルを全て利用する。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IqDfpurbPsI",
        "colab_type": "text"
      },
      "source": [
        "### Decoderの改良1\n",
        "+ Encoderは各単語に対応するLSTMレイヤの隠れ状態ベクトルをhsとして出力\n",
        " + このhsがDecoderに渡されて時系列変換が行われる\n",
        "   + これまでのDecoderはEncoderのLSTMレイヤの最後にある隠れ状態だけを利用していた。\n",
        "\n",
        "+ 入力と出力でどの単語が関連しているのかという対応関係をseq2seqに学習させることはできないか。\n",
        " + 必要な情報にだけ注意を向けさせ、その情報から時系列変換を行うことを目指す。\n",
        "   + この点においてもPeekyとは違う。\n",
        "\n",
        "+ 新たに何らかの計算を行うレイヤを追加する\n",
        " + 何らかの計算\n",
        "   + 各時刻においてLSTMレイヤの隠れ状態とEncoderからのhsを受け取る\n",
        "     + 必要な情報だけを選び出す\n",
        "       + それをその先のAffineレイヤへと出力\n",
        "\n",
        "\n",
        "+ 単語のアラインメント抽出\n",
        " + 各時刻においてDecoderへの入力単語と対応関係にある単語のベクトルをhsから選び出す\n",
        "   + ただし、選び出すという操作に関して微分ができない\n",
        "     + ひとつを選ぶのではなく、全てを選ぶというアプローチ\n",
        "       + 各単語の重要度を表す重みを別途計算する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMQKpYrFETXA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "70dedde1-f782-4465-d5e5-7e84f94f1eb6"
      },
      "source": [
        "!git clone https://github.com/oreilly-japan/deep-learning-from-scratch-2.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'deep-learning-from-scratch-2'...\n",
            "remote: Enumerating objects: 13, done.\u001b[K\n",
            "remote: Counting objects:   7% (1/13)\u001b[K\rremote: Counting objects:  15% (2/13)\u001b[K\rremote: Counting objects:  23% (3/13)\u001b[K\rremote: Counting objects:  30% (4/13)\u001b[K\rremote: Counting objects:  38% (5/13)\u001b[K\rremote: Counting objects:  46% (6/13)\u001b[K\rremote: Counting objects:  53% (7/13)\u001b[K\rremote: Counting objects:  61% (8/13)\u001b[K\rremote: Counting objects:  69% (9/13)\u001b[K\rremote: Counting objects:  76% (10/13)\u001b[K\rremote: Counting objects:  84% (11/13)\u001b[K\rremote: Counting objects:  92% (12/13)\u001b[K\rremote: Counting objects: 100% (13/13)\u001b[K\rremote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 386 (delta 5), reused 10 (delta 5), pack-reused 373\u001b[K\n",
            "Receiving objects: 100% (386/386), 7.91 MiB | 5.40 MiB/s, done.\n",
            "Resolving deltas: 100% (215/215), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDQqP0AohpVF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d29b3dc8-7e42-448a-d4ba-39653d54f135"
      },
      "source": [
        "cd deep-learning-from-scratch-2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/deep-learning-from-scratch-2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iswkSARhsX5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9e58f886-800d-43be-ae8f-22c1b92e1489"
      },
      "source": [
        "ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mch01\u001b[0m/  \u001b[01;34mch03\u001b[0m/  \u001b[01;34mch05\u001b[0m/  \u001b[01;34mch07\u001b[0m/  \u001b[01;34mcommon\u001b[0m/   LICENSE.md\n",
            "\u001b[01;34mch02\u001b[0m/  \u001b[01;34mch04\u001b[0m/  \u001b[01;34mch06\u001b[0m/  \u001b[01;34mch08\u001b[0m/  \u001b[01;34mdataset\u001b[0m/  README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DC7uS6G1hstA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6fc26d12-935f-43b8-e5fe-e78024b68aa3"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 時系列の長さ、隠れ状態ベクトルの要素数\n",
        "T, H = 5, 4\n",
        "hs = np.random.randn(T, H)\n",
        "a = np.array([0.8, 0.1, 0.03, 0.05, 0.02])\n",
        "\n",
        "# repeatで多次元配列を生成\n",
        "ar = a.reshape(5, 1).repeat(4, axis=1)\n",
        "\n",
        "print(hs.shape)\n",
        "print(ar.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5, 4)\n",
            "(5, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMH8wwn1iA43",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "20fd74ba-15a3-414a-accf-a672e6c69607"
      },
      "source": [
        "hs"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.71939546, -0.26841311, -0.68475433,  0.23784718],\n",
              "       [-0.08209138,  0.96028409,  0.44949597, -0.00673877],\n",
              "       [-0.612235  ,  0.76835103,  1.1587083 ,  0.72200734],\n",
              "       [ 2.0682823 ,  0.39065541,  0.40748505, -1.48837174],\n",
              "       [-0.8212756 ,  0.85063449, -1.2713689 ,  3.18008492]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHg-7GWZiMs5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "ffcd7333-6e1c-4458-9483-f2322fc5287a"
      },
      "source": [
        "ar"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.8 , 0.8 , 0.8 , 0.8 ],\n",
              "       [0.1 , 0.1 , 0.1 , 0.1 ],\n",
              "       [0.03, 0.03, 0.03, 0.03],\n",
              "       [0.05, 0.05, 0.05, 0.05],\n",
              "       [0.02, 0.02, 0.02, 0.02]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV9N5UwMiNHs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 重み付け\n",
        "t = hs * ar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrGoXcgWiV_1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "abe18f9d-0288-4215-bda5-9c88382bba83"
      },
      "source": [
        "print(t.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLmmNG3YiY2q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d981a3e0-8be7-48fe-fc4b-c7f50637d1e9"
      },
      "source": [
        "# その重み付けしたものの和\n",
        "c = np.sum(t, axis=0)\n",
        "print(c.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWb4K0qfidd7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "51fe2831-93f7-48bf-bd8f-80c115318a87"
      },
      "source": [
        "c"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.43592879, -0.05910609, -0.47314574,  0.2004472 ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-Wk0ZHkie6g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e6e07afd-c0c1-411f-a433-004773d98de0"
      },
      "source": [
        "# バッチ処理版の重み付き和の実装\n",
        "\n",
        "# バッチの数、時系列の長さ、隠れ状態ベクトルの要素数\n",
        "N, T, H = 10, 5, 4\n",
        "hs = np.random.randn(N, T, H)\n",
        "a = np.random.randn(N, T)\n",
        "ar = a.reshape(N, T, 1).repeat(H, axis=2)\n",
        "\n",
        "t = hs * ar\n",
        "print(t.shape)\n",
        "\n",
        "# バッチごとの隠れ状態のベクトルの要素数ごとの重み付けの合計値\n",
        "c = np.sum(t, axis=1)\n",
        "print(c.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 5, 4)\n",
            "(10, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-r32FxJOn6PJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# コンテキストベクトルをもとめるWeightSumレイヤ\n",
        "class WeightSum:\n",
        "  def __init__(self):\n",
        "    self.params, self.grads = [], []\n",
        "    self.cache = None\n",
        "\n",
        "  def forward(self, hs, a):\n",
        "    N, T, H = hs.shape\n",
        "\n",
        "    ar = a.reshape(N, T, 1).repeat(H, axis=2)\n",
        "    t = hs*ar\n",
        "    c = np.sum(t, axis=1)\n",
        "\n",
        "    self.cache = (hs, ar)\n",
        "    return c\n",
        "\n",
        "  def backward(self, dc):\n",
        "    hs, ar = self.cache\n",
        "    N, T, H = hs.shape\n",
        "\n",
        "    dt = dc.reshape(N, 1, H).repeat(T, axis=1) # sumの逆伝播\n",
        "    dar = dt*hs\n",
        "    dhs = dt*ar\n",
        "    da = np.sum(dar, axis=2) # repeatの逆伝播\n",
        "\n",
        "    return dhs, da"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzQK5enrpqnz",
        "colab_type": "text"
      },
      "source": [
        "### Decoderの改良2\n",
        "+ 各単語の重要度を表す重みaを計算したい。それをもとにコンテキストベクトルを計算できる。\n",
        " + DecoderのLSTMレイヤの隠れ状態ベクトルhが、Encoderの出力であるhsの各単語ベクトルとどれだけ似ているかを数値で表すこと。\n",
        "   + 最も簡単なのはベクトルの内積を利用するもの。\n",
        "\n",
        "$$a \\cdot\tb = a_1b_1 + a_2b_2 + \\dots + a_nb_n $$\n",
        "\n",
        "\n",
        "+ 類似度はsoftmax関数で正規化する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6xnniAfpebJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c1fe5095-2f55-4c04-b102-8a9d9213dce4"
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.layers import Softmax\n",
        "import numpy as np\n",
        "\n",
        "N, T, H = 10, 5, 4\n",
        "hs = np.random.randn(N, T, H)\n",
        "h = np.random.randn(N, H)\n",
        "hr = h.reshape(N, 1, H).repeat(T, axis=1)\n",
        "\n",
        "t = hs*hr\n",
        "print(t.shape)\n",
        "\n",
        "s = np.sum(t, axis=2)\n",
        "print(s.shape)\n",
        "\n",
        "softmax = Softmax()\n",
        "a = softmax.forward(s)\n",
        "print(a.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 5, 4)\n",
            "(10, 5)\n",
            "(10, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHqiOztlW2Z6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.np import *\n",
        "from common.layers import Softmax\n",
        "\n",
        "class AttentionWeight:\n",
        "  def __init__(self):\n",
        "    self.params, self.grads = [], []\n",
        "    self.softmax = Softmax()\n",
        "    self.cache = None\n",
        "\n",
        "  def forward(self, hs, h):\n",
        "    N, T, H = hs.shape\n",
        "\n",
        "    hr = h.reshape(N, 1, H).repeat(T, axis=1)\n",
        "    t = hs*hr\n",
        "    s = np.sum(t, axis=2)\n",
        "    a = self.softmax.forward(s)\n",
        "\n",
        "    self.cache = (hs, hr)\n",
        "    return a\n",
        "\n",
        "  def backward(self, da):\n",
        "    hs, hr = self.cache\n",
        "    N, T, H = hs.shape\n",
        "\n",
        "    ds = self.softmax.backward(da)\n",
        "    dhs = dt*hr\n",
        "    dhr = dt*hs\n",
        "    dh = np.sum(dhr, axis=1)\n",
        "\n",
        "    return dhs, dh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JR504nb2adIc",
        "colab_type": "text"
      },
      "source": [
        "### Decoderの改良3\n",
        "+ Attention WeightレイヤとWeight Sumレイヤを組み合わせることでコンテキストベクトルを求めれるようになる。\n",
        " + Attention WeightレイヤがEncoderが出力する各単語のベクトルhsに対して注意をはらい、各単語の重みaを計算する。\n",
        "  + Weight Sumレイヤがaとhsの重み付き和を求めて、コンテキストベクトルcとして出力する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mqy_YRYOYhvE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention:\n",
        "  def __init__(self):\n",
        "    self.params, self.grads = [], []\n",
        "    self.attention_weight_layer = AttentionWeight()\n",
        "    self.weight_sum_layer = WeightSum()\n",
        "    self.attention_weight = None\n",
        "\n",
        "  def forward(self, hs, h):\n",
        "    a = self.attention_weight_layer.forward(hs, h)\n",
        "    out = self.weight_sum_layer.forward(hs, a)\n",
        "    self.attention_weight = a\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dhs0, da = self.weight_sum_layer.backward(dout)\n",
        "    dhs1, dh = self.attention_weight_layer.backward(da)\n",
        "    dhs = dhs0 + dhs1\n",
        "    return dhs, dh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDUD6M-TezI1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TimeAttention:\n",
        "  def __init__(self):\n",
        "    self.params, self.grads = [], []\n",
        "    self.layers = None\n",
        "    self.attention_weights = None # 各Attentionレイヤの各単語への重み\n",
        "\n",
        "  # 順伝播\n",
        "  def forward(self, hs_enc, hs_dec):\n",
        "    # T個分のAttentionレイヤを作成する\n",
        "    N, T, H = hs_dec.shape\n",
        "    out = np.empty_like(hs_dec)\n",
        "    self.layers = []\n",
        "    self.attention_weights = []\n",
        "\n",
        "    for t in range(T):\n",
        "      layer = Attention()\n",
        "      out[:, t, :] = layer.forward(hs_enc, hs_dec[:, t, :])\n",
        "      self.layers.append(layer)\n",
        "      self.attention_weights.append(layer.attention_weight)\n",
        "\n",
        "    return out\n",
        "\n",
        "  # 逆伝播\n",
        "  def backward(self, dout):\n",
        "    # T個分のAttentionレイヤを作成する\n",
        "    N, T, H = dout.shape\n",
        "    dhs_enc = 0\n",
        "    dhs_dec = np.empty_like(dout)\n",
        "\n",
        "    for t in range(T):\n",
        "      layer = self.layers[t]\n",
        "      dhs, dh = layer.backward(dout[:, t, :])\n",
        "      dhs_enc += dhs\n",
        "      dhs_dec[:, t, :] = dh\n",
        "\n",
        "    return dhs_enc, dhs_dec\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIVdolZji85u",
        "colab_type": "text"
      },
      "source": [
        "## Attention付きのseq2seqの実装"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaVCKpCCjajp",
        "colab_type": "text"
      },
      "source": [
        "### Encoderの実装"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-YsVNn6g_dr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.time_layers import *\n",
        "from ch07.seq2seq import Encoder, Seq2seq\n",
        "from ch08.attention_layer import TimeAttention\n",
        "\n",
        "class AttentionEncoder(Encoder):\n",
        "  def forward(self, xs):\n",
        "    xs = self.embed.forward(xs)\n",
        "    hs = self.lstm.forward(xs)\n",
        "    return hs\n",
        "\n",
        "  def backward(self, dhs):\n",
        "    dout = self.lstm.backward(dhs)\n",
        "    dout = self.embed.backward(dout)\n",
        "    return dout"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxpWYr7-kk47",
        "colab_type": "text"
      },
      "source": [
        "### Decoderの実装"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sqmq-vw-kTzN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttentionDecoder:\n",
        "  def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "    V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "    rn = np.random.randn\n",
        "\n",
        "    embed_W = (rn(V, D) / 100).astype('f')\n",
        "    lstm_Wx = (rn(D, 4*H) / np.sqrt(D) ).astype('f')\n",
        "    lstm_Wh = (rn(H, 4*H) / np.sqrt(H) ).astype('f')\n",
        "    lstm_b = np.zeros(4*H).astype('f')\n",
        "    affine_W = (rn(2*H, V) / np.sqrt(2*H)).astype('f')\n",
        "    affine_b = np.zeros(V).astype('f')\n",
        "\n",
        "    self.embed = TimeEmbedding(embed_W)\n",
        "    self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
        "    self.attention = TimeAttention()\n",
        "    self.affine = TimeAffine(affine_W, affine_b)\n",
        "    layers = [self.embed, self.lstm, self.attention, self.affine]\n",
        "\n",
        "    self.params, self.grads = [], []\n",
        "    for layer in layers:\n",
        "      self.params += layer.params\n",
        "      self.grads += layer.grads\n",
        "\n",
        "  def forward(self, xs, enc_hs):\n",
        "    h = enc_hs[:, -1]\n",
        "    self.lstm.set_state(h)\n",
        "\n",
        "    out = self.embed.forward(out)\n",
        "    # LSTMレイヤ\n",
        "    dec_hs = self.lstm.forward(out)\n",
        "    # TimeAttentionレイヤ\n",
        "    c = self.attention.forward(enc_hs, dec_hs)\n",
        "\n",
        "    # TimeAttentionレイヤとLSTMレイヤの出力を結合\n",
        "    out = np.concatenate((c, dec_hs), axis=2)\n",
        "    score = self.affine.forward(out)\n",
        "\n",
        "    return score\n",
        "\n",
        "  def backward(self, dscore):\n",
        "    dout = self.affine.backward(dscore)\n",
        "    N, T, H2 = dout.shape\n",
        "    H = H2 // 2\n",
        "\n",
        "    dc, ddec_hs0 = dout[:, :, :H], dout[:, :, H:]\n",
        "    denc_hs, ddec_hs1 = self.attention.backward(dc)\n",
        "    ddec_hs = ddec_hs0 + ddec_hs1\n",
        "    dout = self.lstm.dh\n",
        "    denc_hs[:, -1] += dh\n",
        "    self.embed.backward(dout)\n",
        "\n",
        "    return denc_hs\n",
        "\n",
        "  def generate(self, emc_hs, start_id, sample_size):\n",
        "    sampled = []\n",
        "    sample_id = start_id\n",
        "    h = enc_hs[:, -1]\n",
        "    self.lstm.set_state(h)\n",
        "\n",
        "    for _ in range(sample_size):\n",
        "      x = np.array([sample_id]).reshape((1, 1))\n",
        "\n",
        "      out = self.embed.forward(x)\n",
        "      dec_hs = self.lstm.forward(out)\n",
        "      c = self.attention.forward(enc_hs, dec_hs)\n",
        "      out = np.concatenate((c, dec_hs), axis=2)\n",
        "      score = sel.affine.forward(out)\n",
        "\n",
        "      sample_id = np.argmax(score.flatten())\n",
        "      sampled.append(sample_id)\n",
        "    return sampled\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJefVyshrZJZ",
        "colab_type": "text"
      },
      "source": [
        "### seq2seqの実装"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lIXdqjVqHA5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from ch07.seq2seq import Encoder, Seq2seq\n",
        "\n",
        "class AttentionSeq2seq(Seq2seq):\n",
        "  def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "    args = vocab_size, wordvec_size ,hidden_size\n",
        "    self.encoder = AttentionEncoder(*args)\n",
        "    self.decoder = AttentionDecoder(*args)\n",
        "    self.softmax = TimeSoftmaxWithLoss()\n",
        "\n",
        "    self.params = self.encoder.params + self.decoder.params\n",
        "    self.grads = self.encoder.grads + self.decoder.grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvOpA3CLsumP",
        "colab_type": "text"
      },
      "source": [
        "## Attentionの評価\n",
        "+ 日付フォーマットを変換する問題を扱う"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Suky7DY9tBiC",
        "colab_type": "text"
      },
      "source": [
        "### 日付フォーマットの変換問題"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqqW8IgWtj_z",
        "colab_type": "text"
      },
      "source": [
        "### Attention付きseq2seqの学習"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UG7PbMqx5yd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.time_layers import *\n",
        "from ch07.seq2seq import Seq2seq, Encoder\n",
        "\n",
        "\n",
        "class PeekyDecoder:\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        embed_W = (rn(V, D) / 100).astype('f')\n",
        "        lstm_Wx = (rn(H + D, 4 * H) / np.sqrt(H + D)).astype('f')\n",
        "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b = np.zeros(4 * H).astype('f')\n",
        "        affine_W = (rn(H + H, V) / np.sqrt(H + H)).astype('f')\n",
        "        affine_b = np.zeros(V).astype('f')\n",
        "\n",
        "        self.embed = TimeEmbedding(embed_W)\n",
        "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
        "        self.affine = TimeAffine(affine_W, affine_b)\n",
        "\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in (self.embed, self.lstm, self.affine):\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, xs, h):\n",
        "        N, T = xs.shape\n",
        "        N, H = h.shape\n",
        "\n",
        "        self.lstm.set_state(h)\n",
        "\n",
        "        out = self.embed.forward(xs)\n",
        "        hs = np.repeat(h, T, axis=0).reshape(N, T, H)\n",
        "        out = np.concatenate((hs, out), axis=2)\n",
        "\n",
        "        out = self.lstm.forward(out)\n",
        "        out = np.concatenate((hs, out), axis=2)\n",
        "\n",
        "        score = self.affine.forward(out)\n",
        "        self.cache = H\n",
        "        return score\n",
        "\n",
        "    def backward(self, dscore):\n",
        "        H = self.cache\n",
        "\n",
        "        dout = self.affine.backward(dscore)\n",
        "        dout, dhs0 = dout[:, :, H:], dout[:, :, :H]\n",
        "        dout = self.lstm.backward(dout)\n",
        "        dembed, dhs1 = dout[:, :, H:], dout[:, :, :H]\n",
        "        self.embed.backward(dembed)\n",
        "\n",
        "        dhs = dhs0 + dhs1\n",
        "        dh = self.lstm.dh + np.sum(dhs, axis=1)\n",
        "        return dh\n",
        "\n",
        "    def generate(self, h, start_id, sample_size):\n",
        "        sampled = []\n",
        "        char_id = start_id\n",
        "        self.lstm.set_state(h)\n",
        "\n",
        "        H = h.shape[1]\n",
        "        peeky_h = h.reshape(1, 1, H)\n",
        "        for _ in range(sample_size):\n",
        "            x = np.array([char_id]).reshape((1, 1))\n",
        "            out = self.embed.forward(x)\n",
        "\n",
        "            out = np.concatenate((peeky_h, out), axis=2)\n",
        "            out = self.lstm.forward(out)\n",
        "            out = np.concatenate((peeky_h, out), axis=2)\n",
        "            score = self.affine.forward(out)\n",
        "\n",
        "            char_id = np.argmax(score.flatten())\n",
        "            sampled.append(char_id)\n",
        "\n",
        "        return sampled\n",
        "\n",
        "\n",
        "class PeekySeq2seq(Seq2seq):\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        self.encoder = Encoder(V, D, H)\n",
        "        self.decoder = PeekyDecoder(V, D, H)\n",
        "        self.softmax = TimeSoftmaxWithLoss()\n",
        "\n",
        "        self.params = self.encoder.params + self.decoder.params\n",
        "        self.grads = self.encoder.grads + self.decoder.grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sb5MYr7RsoPx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "19c540b9-c9f0-4eab-9ee8-066fa1a6ed64"
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "import numpy as np\n",
        "from dataset import sequence\n",
        "from common.optimizer import Adam\n",
        "from common.trainer import Trainer\n",
        "from common.util import eval_seq2seq\n",
        "from ch08.attention_seq2seq import AttentionSeq2seq\n",
        "# from ch07.seq2seq import Seq2seq\n",
        "# from ch07.peeky_seq2seq import PeekySeq2seq\n",
        "\n",
        "# データの読み込み\n",
        "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
        "char_to_id, id_to_char = sequence.get_vocab()\n",
        "\n",
        "# 入力文を反転\n",
        "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
        "\n",
        "# ハイパーパラメータの設定\n",
        "vocab_size = len(char_to_id)\n",
        "wordvec_size = 16\n",
        "hidden_size = 256\n",
        "batch_size = 128\n",
        "max_epoch = 10\n",
        "max_grad = 5.0\n",
        "\n",
        "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
        "optimizer = Adam()\n",
        "trainer = Trainer(model, optimizer)\n",
        "\n",
        "acc_list = []\n",
        "for epoch in range(max_epoch):\n",
        "  trainer.fit(x_train, t_train, max_epoch=1,\n",
        "              batch_size=batch_size, max_grad=max_grad)\n",
        "  \n",
        "  correct_num = 0\n",
        "  for i in range(len(x_test)):\n",
        "    question, correct = x_test[[i]], t_test[[i]]\n",
        "    verbose = i < 10\n",
        "    correct_num += eval_seq2seq(model, question, correct,\n",
        "                                    id_to_char, verbose, is_reverse=True)\n",
        "  \n",
        "  acc = float(correct_num) / len(x_test)\n",
        "  acc_list.append(acc)\n",
        "  print('val acc %.3f%%' % (acc * 100))\n",
        "\n",
        "model.save_params()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| epoch 1 |  iter 1 / 351 | time 0[s] | loss 4.08\n",
            "| epoch 1 |  iter 21 / 351 | time 13[s] | loss 3.09\n",
            "| epoch 1 |  iter 41 / 351 | time 27[s] | loss 1.90\n",
            "| epoch 1 |  iter 61 / 351 | time 40[s] | loss 1.72\n",
            "| epoch 1 |  iter 81 / 351 | time 53[s] | loss 1.46\n",
            "| epoch 1 |  iter 101 / 351 | time 67[s] | loss 1.19\n",
            "| epoch 1 |  iter 121 / 351 | time 80[s] | loss 1.14\n",
            "| epoch 1 |  iter 141 / 351 | time 93[s] | loss 1.09\n",
            "| epoch 1 |  iter 161 / 351 | time 106[s] | loss 1.06\n",
            "| epoch 1 |  iter 181 / 351 | time 119[s] | loss 1.04\n",
            "| epoch 1 |  iter 201 / 351 | time 132[s] | loss 1.03\n",
            "| epoch 1 |  iter 221 / 351 | time 146[s] | loss 1.02\n",
            "| epoch 1 |  iter 241 / 351 | time 159[s] | loss 1.02\n",
            "| epoch 1 |  iter 261 / 351 | time 172[s] | loss 1.01\n",
            "| epoch 1 |  iter 281 / 351 | time 185[s] | loss 1.00\n",
            "| epoch 1 |  iter 301 / 351 | time 198[s] | loss 1.00\n",
            "| epoch 1 |  iter 321 / 351 | time 211[s] | loss 1.00\n",
            "| epoch 1 |  iter 341 / 351 | time 224[s] | loss 1.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "val acc 0.000%\n",
            "| epoch 2 |  iter 1 / 351 | time 0[s] | loss 1.00\n",
            "| epoch 2 |  iter 21 / 351 | time 13[s] | loss 1.00\n",
            "| epoch 2 |  iter 41 / 351 | time 26[s] | loss 0.99\n",
            "| epoch 2 |  iter 61 / 351 | time 39[s] | loss 0.99\n",
            "| epoch 2 |  iter 81 / 351 | time 52[s] | loss 0.99\n",
            "| epoch 2 |  iter 101 / 351 | time 65[s] | loss 0.99\n",
            "| epoch 2 |  iter 121 / 351 | time 78[s] | loss 0.99\n",
            "| epoch 2 |  iter 141 / 351 | time 90[s] | loss 0.98\n",
            "| epoch 2 |  iter 161 / 351 | time 103[s] | loss 0.98\n",
            "| epoch 2 |  iter 181 / 351 | time 116[s] | loss 0.97\n",
            "| epoch 2 |  iter 201 / 351 | time 129[s] | loss 0.95\n",
            "| epoch 2 |  iter 221 / 351 | time 142[s] | loss 0.94\n",
            "| epoch 2 |  iter 241 / 351 | time 155[s] | loss 0.90\n",
            "| epoch 2 |  iter 261 / 351 | time 168[s] | loss 0.83\n",
            "| epoch 2 |  iter 281 / 351 | time 181[s] | loss 0.74\n",
            "| epoch 2 |  iter 301 / 351 | time 194[s] | loss 0.66\n",
            "| epoch 2 |  iter 321 / 351 | time 207[s] | loss 0.58\n",
            "| epoch 2 |  iter 341 / 351 | time 220[s] | loss 0.46\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[91m☒\u001b[0m 2006-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[91m☒\u001b[0m 2007-08-09\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[91m☒\u001b[0m 1983-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[91m☒\u001b[0m 2016-11-08\n",
            "---\n",
            "val acc 51.680%\n",
            "| epoch 3 |  iter 1 / 351 | time 0[s] | loss 0.35\n",
            "| epoch 3 |  iter 21 / 351 | time 13[s] | loss 0.30\n",
            "| epoch 3 |  iter 41 / 351 | time 26[s] | loss 0.21\n",
            "| epoch 3 |  iter 61 / 351 | time 39[s] | loss 0.14\n",
            "| epoch 3 |  iter 81 / 351 | time 52[s] | loss 0.09\n",
            "| epoch 3 |  iter 101 / 351 | time 65[s] | loss 0.07\n",
            "| epoch 3 |  iter 121 / 351 | time 78[s] | loss 0.05\n",
            "| epoch 3 |  iter 141 / 351 | time 91[s] | loss 0.04\n",
            "| epoch 3 |  iter 161 / 351 | time 104[s] | loss 0.03\n",
            "| epoch 3 |  iter 181 / 351 | time 116[s] | loss 0.03\n",
            "| epoch 3 |  iter 201 / 351 | time 129[s] | loss 0.02\n",
            "| epoch 3 |  iter 221 / 351 | time 142[s] | loss 0.02\n",
            "| epoch 3 |  iter 241 / 351 | time 155[s] | loss 0.02\n",
            "| epoch 3 |  iter 261 / 351 | time 168[s] | loss 0.01\n",
            "| epoch 3 |  iter 281 / 351 | time 181[s] | loss 0.01\n",
            "| epoch 3 |  iter 301 / 351 | time 194[s] | loss 0.01\n",
            "| epoch 3 |  iter 321 / 351 | time 207[s] | loss 0.01\n",
            "| epoch 3 |  iter 341 / 351 | time 219[s] | loss 0.01\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "val acc 99.900%\n",
            "| epoch 4 |  iter 1 / 351 | time 0[s] | loss 0.01\n",
            "| epoch 4 |  iter 21 / 351 | time 13[s] | loss 0.01\n",
            "| epoch 4 |  iter 41 / 351 | time 26[s] | loss 0.01\n",
            "| epoch 4 |  iter 61 / 351 | time 39[s] | loss 0.01\n",
            "| epoch 4 |  iter 81 / 351 | time 52[s] | loss 0.01\n",
            "| epoch 4 |  iter 101 / 351 | time 65[s] | loss 0.01\n",
            "| epoch 4 |  iter 121 / 351 | time 78[s] | loss 0.00\n",
            "| epoch 4 |  iter 141 / 351 | time 91[s] | loss 0.01\n",
            "| epoch 4 |  iter 161 / 351 | time 104[s] | loss 0.00\n",
            "| epoch 4 |  iter 181 / 351 | time 116[s] | loss 0.00\n",
            "| epoch 4 |  iter 201 / 351 | time 129[s] | loss 0.00\n",
            "| epoch 4 |  iter 221 / 351 | time 142[s] | loss 0.00\n",
            "| epoch 4 |  iter 241 / 351 | time 155[s] | loss 0.00\n",
            "| epoch 4 |  iter 261 / 351 | time 168[s] | loss 0.00\n",
            "| epoch 4 |  iter 281 / 351 | time 180[s] | loss 0.00\n",
            "| epoch 4 |  iter 301 / 351 | time 193[s] | loss 0.00\n",
            "| epoch 4 |  iter 321 / 351 | time 206[s] | loss 0.00\n",
            "| epoch 4 |  iter 341 / 351 | time 219[s] | loss 0.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "val acc 99.900%\n",
            "| epoch 5 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
            "| epoch 5 |  iter 21 / 351 | time 13[s] | loss 0.00\n",
            "| epoch 5 |  iter 41 / 351 | time 26[s] | loss 0.00\n",
            "| epoch 5 |  iter 61 / 351 | time 39[s] | loss 0.00\n",
            "| epoch 5 |  iter 81 / 351 | time 52[s] | loss 0.00\n",
            "| epoch 5 |  iter 101 / 351 | time 65[s] | loss 0.00\n",
            "| epoch 5 |  iter 121 / 351 | time 78[s] | loss 0.00\n",
            "| epoch 5 |  iter 141 / 351 | time 90[s] | loss 0.00\n",
            "| epoch 5 |  iter 161 / 351 | time 103[s] | loss 0.00\n",
            "| epoch 5 |  iter 181 / 351 | time 116[s] | loss 0.00\n",
            "| epoch 5 |  iter 201 / 351 | time 129[s] | loss 0.00\n",
            "| epoch 5 |  iter 221 / 351 | time 142[s] | loss 0.00\n",
            "| epoch 5 |  iter 241 / 351 | time 154[s] | loss 0.00\n",
            "| epoch 5 |  iter 261 / 351 | time 167[s] | loss 0.00\n",
            "| epoch 5 |  iter 281 / 351 | time 180[s] | loss 0.00\n",
            "| epoch 5 |  iter 301 / 351 | time 193[s] | loss 0.00\n",
            "| epoch 5 |  iter 321 / 351 | time 206[s] | loss 0.00\n",
            "| epoch 5 |  iter 341 / 351 | time 219[s] | loss 0.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "val acc 99.920%\n",
            "| epoch 6 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
            "| epoch 6 |  iter 21 / 351 | time 13[s] | loss 0.00\n",
            "| epoch 6 |  iter 41 / 351 | time 26[s] | loss 0.00\n",
            "| epoch 6 |  iter 61 / 351 | time 39[s] | loss 0.00\n",
            "| epoch 6 |  iter 81 / 351 | time 52[s] | loss 0.00\n",
            "| epoch 6 |  iter 101 / 351 | time 65[s] | loss 0.00\n",
            "| epoch 6 |  iter 121 / 351 | time 77[s] | loss 0.00\n",
            "| epoch 6 |  iter 141 / 351 | time 90[s] | loss 0.00\n",
            "| epoch 6 |  iter 161 / 351 | time 103[s] | loss 0.00\n",
            "| epoch 6 |  iter 181 / 351 | time 116[s] | loss 0.00\n",
            "| epoch 6 |  iter 201 / 351 | time 129[s] | loss 0.00\n",
            "| epoch 6 |  iter 221 / 351 | time 142[s] | loss 0.00\n",
            "| epoch 6 |  iter 241 / 351 | time 155[s] | loss 0.00\n",
            "| epoch 6 |  iter 261 / 351 | time 167[s] | loss 0.00\n",
            "| epoch 6 |  iter 281 / 351 | time 180[s] | loss 0.00\n",
            "| epoch 6 |  iter 301 / 351 | time 193[s] | loss 0.00\n",
            "| epoch 6 |  iter 321 / 351 | time 206[s] | loss 0.00\n",
            "| epoch 6 |  iter 341 / 351 | time 219[s] | loss 0.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[91m☒\u001b[0m 1994-05-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[91m☒\u001b[0m 2013-08-22\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[91m☒\u001b[0m 2016-01-06\n",
            "---\n",
            "val acc 81.040%\n",
            "| epoch 7 |  iter 1 / 351 | time 0[s] | loss 0.11\n",
            "| epoch 7 |  iter 21 / 351 | time 13[s] | loss 0.03\n",
            "| epoch 7 |  iter 41 / 351 | time 26[s] | loss 0.01\n",
            "| epoch 7 |  iter 61 / 351 | time 38[s] | loss 0.00\n",
            "| epoch 7 |  iter 81 / 351 | time 51[s] | loss 0.00\n",
            "| epoch 7 |  iter 101 / 351 | time 64[s] | loss 0.00\n",
            "| epoch 7 |  iter 121 / 351 | time 77[s] | loss 0.00\n",
            "| epoch 7 |  iter 141 / 351 | time 90[s] | loss 0.00\n",
            "| epoch 7 |  iter 161 / 351 | time 103[s] | loss 0.00\n",
            "| epoch 7 |  iter 181 / 351 | time 115[s] | loss 0.00\n",
            "| epoch 7 |  iter 201 / 351 | time 128[s] | loss 0.00\n",
            "| epoch 7 |  iter 221 / 351 | time 141[s] | loss 0.00\n",
            "| epoch 7 |  iter 241 / 351 | time 153[s] | loss 0.00\n",
            "| epoch 7 |  iter 261 / 351 | time 166[s] | loss 0.00\n",
            "| epoch 7 |  iter 281 / 351 | time 179[s] | loss 0.00\n",
            "| epoch 7 |  iter 301 / 351 | time 192[s] | loss 0.00\n",
            "| epoch 7 |  iter 321 / 351 | time 204[s] | loss 0.00\n",
            "| epoch 7 |  iter 341 / 351 | time 217[s] | loss 0.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "val acc 100.000%\n",
            "| epoch 8 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
            "| epoch 8 |  iter 21 / 351 | time 14[s] | loss 0.00\n",
            "| epoch 8 |  iter 41 / 351 | time 27[s] | loss 0.00\n",
            "| epoch 8 |  iter 61 / 351 | time 41[s] | loss 0.00\n",
            "| epoch 8 |  iter 81 / 351 | time 54[s] | loss 0.00\n",
            "| epoch 8 |  iter 101 / 351 | time 67[s] | loss 0.00\n",
            "| epoch 8 |  iter 121 / 351 | time 81[s] | loss 0.00\n",
            "| epoch 8 |  iter 141 / 351 | time 94[s] | loss 0.00\n",
            "| epoch 8 |  iter 161 / 351 | time 108[s] | loss 0.00\n",
            "| epoch 8 |  iter 181 / 351 | time 121[s] | loss 0.00\n",
            "| epoch 8 |  iter 201 / 351 | time 135[s] | loss 0.00\n",
            "| epoch 8 |  iter 221 / 351 | time 148[s] | loss 0.00\n",
            "| epoch 8 |  iter 241 / 351 | time 162[s] | loss 0.00\n",
            "| epoch 8 |  iter 261 / 351 | time 176[s] | loss 0.00\n",
            "| epoch 8 |  iter 281 / 351 | time 189[s] | loss 0.00\n",
            "| epoch 8 |  iter 301 / 351 | time 203[s] | loss 0.00\n",
            "| epoch 8 |  iter 321 / 351 | time 216[s] | loss 0.00\n",
            "| epoch 8 |  iter 341 / 351 | time 230[s] | loss 0.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "val acc 100.000%\n",
            "| epoch 9 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
            "| epoch 9 |  iter 21 / 351 | time 14[s] | loss 0.00\n",
            "| epoch 9 |  iter 41 / 351 | time 28[s] | loss 0.00\n",
            "| epoch 9 |  iter 61 / 351 | time 41[s] | loss 0.00\n",
            "| epoch 9 |  iter 81 / 351 | time 55[s] | loss 0.00\n",
            "| epoch 9 |  iter 101 / 351 | time 68[s] | loss 0.00\n",
            "| epoch 9 |  iter 121 / 351 | time 82[s] | loss 0.00\n",
            "| epoch 9 |  iter 141 / 351 | time 95[s] | loss 0.00\n",
            "| epoch 9 |  iter 161 / 351 | time 108[s] | loss 0.00\n",
            "| epoch 9 |  iter 181 / 351 | time 122[s] | loss 0.00\n",
            "| epoch 9 |  iter 201 / 351 | time 135[s] | loss 0.00\n",
            "| epoch 9 |  iter 221 / 351 | time 148[s] | loss 0.00\n",
            "| epoch 9 |  iter 241 / 351 | time 161[s] | loss 0.00\n",
            "| epoch 9 |  iter 261 / 351 | time 175[s] | loss 0.00\n",
            "| epoch 9 |  iter 281 / 351 | time 189[s] | loss 0.00\n",
            "| epoch 9 |  iter 301 / 351 | time 202[s] | loss 0.00\n",
            "| epoch 9 |  iter 321 / 351 | time 216[s] | loss 0.00\n",
            "| epoch 9 |  iter 341 / 351 | time 230[s] | loss 0.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "val acc 100.000%\n",
            "| epoch 10 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
            "| epoch 10 |  iter 21 / 351 | time 14[s] | loss 0.00\n",
            "| epoch 10 |  iter 41 / 351 | time 28[s] | loss 0.00\n",
            "| epoch 10 |  iter 61 / 351 | time 41[s] | loss 0.00\n",
            "| epoch 10 |  iter 81 / 351 | time 54[s] | loss 0.00\n",
            "| epoch 10 |  iter 101 / 351 | time 67[s] | loss 0.00\n",
            "| epoch 10 |  iter 121 / 351 | time 81[s] | loss 0.00\n",
            "| epoch 10 |  iter 141 / 351 | time 94[s] | loss 0.00\n",
            "| epoch 10 |  iter 161 / 351 | time 107[s] | loss 0.00\n",
            "| epoch 10 |  iter 181 / 351 | time 120[s] | loss 0.00\n",
            "| epoch 10 |  iter 201 / 351 | time 133[s] | loss 0.00\n",
            "| epoch 10 |  iter 221 / 351 | time 146[s] | loss 0.00\n",
            "| epoch 10 |  iter 241 / 351 | time 159[s] | loss 0.00\n",
            "| epoch 10 |  iter 261 / 351 | time 172[s] | loss 0.00\n",
            "| epoch 10 |  iter 281 / 351 | time 185[s] | loss 0.00\n",
            "| epoch 10 |  iter 301 / 351 | time 198[s] | loss 0.00\n",
            "| epoch 10 |  iter 321 / 351 | time 211[s] | loss 0.00\n",
            "| epoch 10 |  iter 341 / 351 | time 224[s] | loss 0.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "val acc 100.000%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fks3_l5xXLV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "de13ba29-0d08-4d2b-b242-2bfcc92f2181"
      },
      "source": [
        "# グラフの描画\n",
        "x = np.arange(len(acc_list))\n",
        "plt.plot(x, acc_list, marker='o')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.ylim(-0.05, 1.05)\n",
        "plt.show()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68LKx-7gyAgq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rwsWmUP5Fu0",
        "colab_type": "text"
      },
      "source": [
        "### Attentionの可視化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0hRlJhaAwXM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6894131f-114d-45ad-b222-f77e74b91060"
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "import numpy as np\n",
        "from dataset import sequence\n",
        "import matplotlib.pyplot as plt\n",
        "from ch08.attention_seq2seq import AttentionSeq2seq\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
        "char_to_id, id_to_char = sequence.get_vocab()\n",
        "\n",
        "# Reverse input\n",
        "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
        "\n",
        "vocab_size = len(char_to_id)\n",
        "wordvec_size = 16\n",
        "hidden_size = 256\n",
        "\n",
        "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
        "model.load_params()\n",
        "\n",
        "_idx = 0\n",
        "\n",
        "def visualize(attention_map, row_labels, column_labels):\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.pcolor(attention_map, cmap=plt.cm.Greys_r, vmin=0.0, vmax=1.0)\n",
        "\n",
        "  ax.patch.set_facecolor('black')\n",
        "  ax.set_yticks(np.arange(attention_map.shape[0])+0.5, minor=False)\n",
        "  ax.set_xticks(np.arange(attention_map.shape[1])+0.5, minor=False)\n",
        "  ax.invert_yaxis()\n",
        "  ax.set_xticklabels(row_labels, minor=False)\n",
        "  ax.set_yticklabels(column_labels, minor=False)\n",
        "\n",
        "  global _idx\n",
        "  _idx += 1\n",
        "  plt.show()\n",
        "\n",
        "np.random.seed(1984)\n",
        "\n",
        "for _ in range(5):\n",
        "  idx = [np.random.randint(0, len(x_test))]\n",
        "  x = x_test[idx]\n",
        "  t = t_test[idx]\n",
        "\n",
        "  model.forward(x, t)\n",
        "  d = model.decoder.attention.attention_weights\n",
        "  d = np.array(d)\n",
        "  attention_map = d.reshape(d.shape[0], d.shape[2])\n",
        "\n",
        "  # reverse for print\n",
        "  attention_map = attention_map[:, ::-1]\n",
        "  x = x[:, ::-1]\n",
        "\n",
        "  row_labels = [id_to_char[i] for i in x[0]]\n",
        "  column_labels = [id_to_char[i] for i in t[0]]\n",
        "  column_labels = column_labels[1:]\n",
        "\n",
        "  visualize(attention_map, row_labels, column_labels)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAANkklEQVR4nO3da4xdZRXG8ecpBdoiSikXhYDFYCpSw62SAsEYigkSDJVIAgaJsaEfrKEgfDGaeEkwkhC+KF4m0tQLNso1aqIWEWlArNBaYEoFNFyskBQoNC0llDLLD3tPGMo5c/ae2fvMmpn/LznptLPO2zXn7HnmnX17HRECAOQ1Y6IbAACMjqAGgOQIagBIjqAGgOQIagBIbmYbg9rmVBJgkjv11FMr127atKnW2ENDQ3XbmQ5eiojDO33CbZyeR1ADk9+ePXsq1x5yyCG1xt69e3fddqaDDRGxqNMn2PUBAMkR1ACQHEENAMkR1ACQHEENAMkR1ACQXM+gtr3K9jbbg/1oCADwTlVm1KslnddyHwCALnoGdUSsk7S9D70AADpo7BJy28slLW9qPABAobGgjogBSQMSl5ADQJM46wMAkiOoASC5KqfnrZH0oKQFtrfaXtZ+WwCAYT33UUfEpf1oBADQGbs+ACA5ghoAkiOoASA5ghoAkiOoASC5VlYhB5BP3QVoZ82aVbl2xYoVtcZesmRJ5dqlS5fWGnsqYkYNAMkR1ACQHEENAMkR1ACQHEENAMkR1ACQHEENAMlVCmrbK20P2t5s+6q2mwIAvK3K/agXSrpC0umSTpJ0ge3j224MAFCoMqM+QdL6iNgdEXsl3SfponbbAgAMqxLUg5LOtj3P9hxJ50s6Zt8i28ttP2z74aabBIDprMoKL1tsXy9praTXJG2S9FaHOlYhB4AWVDqYGBE3R8RpEfEJSa9IerLdtgAAwyrdPc/2ERGxzfaxKvZPL263LQDAsKq3Ob3d9jxJb0paERGvttgTAGCESkEdEWe33QgAoDOuTASA5AhqAEiOoAaA5AhqAEjOEc1fm8IFLwBGUyd3bLfYSSobImJRp08wowaA5AhqAEiOoAaA5AhqAEiOoAaA5AhqAEiOoAaA5AhqAEiOVcgBIDlWIQeA5FiFHACSYxVyAEiu0k2ZbC+T9GUVq5BvlvRGRHTdV81NmQCMhpsydTS+mzKxCjkATBxWIQeA5FiFHACSYxVyAEiOKxMBIDmCGgCSI6gBIDmCGgCSq3rWBwA0ps5FLHUujqk79mTBjBoAkiOoASA5ghoAkiOoASA5ghoAkiOoASA5ghoAkqu6uO3V5cK2g7bX2J7VdmMAgEKVxW2PlnSlpEURsVDSfpIuabsxAECh6q6PmZJm254paY6k59trCQAwUs+gjoj/SbpB0nOSXpC0IyLW7lvH4rYA0I4quz7mSrpQ0nGSjpJ0kO3L9q2LiIGIWNRtcUYAwNhU2fVxrqSnI+LFiHhT0h2Szmy3LQDAsCpB/ZykxbbnuLgt1RJJW9ptCwAwrMo+6vWSbpO0UdJj5XMGWu4LAFBy3Xu9VhrUbn5QANPSNLof9YZux/i4MhEAkiOoASA5ghoAkiOoASA5ghoAkptUq5AfcMABtep37txZufbAAw+s2w7Gqc2VqDF1zJhRbz45FbcrZtQAkBxBDQDJEdQAkBxBDQDJEdQAkBxBDQDJEdQAkBxBDQDJEdQAkBxBDQDJNXYJue3lkpY3NR4AoDCpVnjhXh9Ty1S8JwOa1+aKLcm2q/Gv8GJ7he1N5eOo5noDAIyGGXWJGXX/MaNGFcyoOZgIAOkR1ACQHEENAMkR1ACQHEENAMkR1ACQHEENAMlNqlXI9+zZU6t+sp4bnen84qGhocq1dVeLTnYOK5JiO2FGDQDpEdQAkBxBDQDJEdQAkBxBDQDJEdQAkBxBDQDJ9Qxq26tsb7M92I+GAADvVGVGvVrSeS33AQDoomdQR8Q6Sdv70AsAoANWIQeA5BoL6ogYkDQgtbdmIgBMR5z1AQDJEdQAkFyV0/PWSHpQ0gLbW20va78tAMCwnvuoI+LSfjQCAOiMXR8AkBxBDQDJEdQAkBxBDQDJTarFbaeLOot51ll8Vqq/AG3d+joyLeILZMaMGgCSI6gBIDmCGgCSI6gBIDmCGgCSI6gBIDmCGgCSI6gBILkqtzk9xva9th+3vdn2yn40BgAoVLkyca+kayJio+2DJW2wfXdEPN5ybwAAVVuF/IWI2Fh+vFPSFklHt90YAKBQ614ftudLOkXS+g6fYxVyAGiBq97sxvZ7JN0n6bqIuKNHLXfQ6ZO2b8rUJm7KBLzDhohY1OkTlb5rbe8v6XZJt/QKaQBAs6qc9WFJN0vaEhE3tt8SAGCkKjPqsyR9QdI5tjeVj/Nb7gsAUKqyCvn9kqrvTAQANCrPkSUAQEcENQAkR1ADQHIENQAkN6VXIZ87d27l2h07dtQau+6FJm3ZuXPnRLcwZgcddFDl2l27drXYCZAbM2oASI6gBoDkCGoASI6gBoDkCGoASI6gBoDkCGoASI6gBoDkCGoASI6gBoDkGruEnMVtAaAdjQV1RAxIGpBY3BYAmlR514ftFSOW4jqqzaYAAG+rPKOOiJsk3dRiLwCADjiYCADJEdQAkBxBDQDJEdQAkBxBDQDJEdQAkBxBDQDJOaL5iwi5MhEo1F2tfsYM5k7T2IaIWNTpE2wVAJAcQQ0AyRHUAJAcQQ0AyRHUAJAcQQ0AyRHUAJBcz6C2vcr2NtuD/WgIAPBOVWbUqyWd13IfAIAuegZ1RKyTtL0PvQAAOmAVcgBIjlXIASA5zvoAgOQIagBIrsrpeWskPShpge2ttpe13xYAYFjPfdQRcWk/GgEAdMauDwBIjqAGgOQIagBIjqAGgOQIagBIrrErEwG8W91VxSOqX9Rru247mKSYUQNAcgQ1ACRHUANAcgQ1ACRHUANAcgQ1ACRHUANAcqxCDgDJsQo5ACTHKuQAkByrkANAcqxCDgDJcdYHACRHUANAcqxCDgDJsQo5ACTHrg8ASI6gBoDkCGoASI6gBoDkCGoASK6tVchfkvTsPv92WPnvVdWpb3PsTL0wdn/H7nsvo6wsnrrvKTj2RPTywa7VEdGXh6SH26pvc+xMvTA27z1jT7/3PiLY9QEA2RHUAJBcP4N6oMX6NseuW8/YU2fsuvWMPXXGrlvfai8u95cAAJJi1wcAJEdQA0ByrQe17bdsbxrxmN/C//G3MTznW7avbbqXiTbi9d5s+xHb19iecj+Qbc+3PTjRfYyF7VW2t1Xpv05t2+r2Ynul7cFyW7yqqdqy/uqydtD2Gtuzqn4dk1E/voFfj4iTRzyeqfNkF0btMyLOHFeHU8vw632ipE9J+rSkb05wT5NalW2wptWSzmuhtm2rVbEX2wslXSHpdEknSbrA9vHjrS3rj5Z0paRFEbFQ0n6SLqn+ZUw+KWda5WzpCds/lzQo6Zge9bsqjvt120/avl/Sggr1d9neUP7k7rpwr+3vjJwF2L7O9soqPbUpIrapWHD4Kx7lkjfbl9n+RzkT/4nt/UYb1/blth8tZ+y/6FHbc+zy/f6X7dXl+3OL7XNtP2D7Kdundxl+Zlm7xfZttuc0+DXW2gbriIh1krY3Xdu2mr2cIGl9ROyOiL2S7pN0UQO1w2ZKmm17pqQ5kp6v2NfkVOfqmLE8JL0laVP5uLPic+ZLGpK0uGL9rgo1p0l6TMWb+l5J/5Z0bY/nHFr+OVvFN+u8UfrdWH48Q9J/utX24fV+12sh6VVJR3apP0HS7yTtX/79h5IuH2X8EyU9Kemwka/ReMYuX7+9kj5Wvn4bJK2SZEkXSrqry3NC0lnl31d1ez/rfo1j2QbH8D7NlzTYdG0ftq9KvZSv+ZOS5pXfcw9K+v54a0c8Z6WkXZJelHTLRL8ubT/autfHSK9HxMljeN6zEfH3Bvs4W8UPit2SZPu3FZ5zpe3Plh8fI+nDkl7etyginrH9su1TJB0p6Z8R8a66pJao+CH2UDnpni1p2yj150i6NSJekqSIGG2GVWfspyPiMUmyvVnSPRERth9TEQ6d/DciHig//qWKX4dvGGcfIzW9DU4bEbHF9vWS1kp6TcVE7a3x1kqS7bkqfoAfp2IScqvtyyLil81+FXn0I6jH6rWJ/M9tf1LSuZLOiIjdtv8qabQDFj+V9EVJ71cxu0vB9odUbPTdgsmSfhYRX2vjv68x9hsjPh4a8fchdd9O970IoNtFAWP9Gid0G5zsIuJmSTdLku3vStraRK2K78unI+LFsv4OSWeq+GE9JaXcR92SdZKW2p5t+2BJn+lR/z5Jr5Qh/RFJi3vU36niQMvHJf2palO27ykPjjTO9uGSfizpB1H+vtjBPZI+Z/uI8jmH2u5+Fy/pL5Iutj1vuH6U2rpj13Ws7TPKjz8v6f4J6iOdNrerGj0Mv97Hqtjn/KsmaiU9J2mx7TnlsZclkrY01XdG0yaoI2KjpF9LekTSHyQ91OMpf1RxsGqLpO9JGvVX4IjYI+leSb+JiK6/to1UnklwvJo9WDS7PGC2WdKfVfw6+e1uxRHxuKRvSFpr+1FJd0v6wCj1myVdJ+k+249IurGpscfgCUkryvdorqQfTVAftdheo2I/7ALbW20va6J2xHPa2K7G0svtth9XcXxgRUS82kRtRKyXdJukjSqOO81Q/Uu4JxUuIW9I+c2xUdLFEfFUxecslPSliPhqq81hWmG7mnoI6gbY/qik36s4WHnNRPcDYGohqAEguWmzjxoAJiuCGgCSI6gBIDmCGgCSI6gBILn/A5YABG+Mq5SaAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAW8yb-UF-Ia",
        "colab_type": "text"
      },
      "source": [
        "## Attentionに関する残りのテーマ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07Ubo9I2GDy9",
        "colab_type": "text"
      },
      "source": [
        "### 双方向RNN\n",
        "+ 文章を左から右に読むだけでなく、右から左に読むなどもしたい。\n",
        "\n",
        "+ 双方向LSTMはこれまでのLSTMレイヤに加えて、逆方向に処理するLSTMレイヤも追加する\n",
        "  + 各時刻において2つのLSTMレイヤの隠れ状態を連結し、それを最終的な隠れ状態ベクトルとする\n",
        "  + 双方向から処理することで、各単語に対応する隠れ状態ベクトルは左と右の両方向から情報を集約することができる。\n",
        "\n",
        "+ 実装方法\n",
        " + 一つのLSTMレイヤはこれまで通りの入力文を与える（左から右方向）\n",
        " + もう一つのLSTMレイヤには入力文を右から左方向に並べ替えたものを与える。\n",
        " + これら二つのLSTMレイヤの出力を連結すれば双方向LSTMレイヤができる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZVZL3-3ILQ3",
        "colab_type": "text"
      },
      "source": [
        "### Attentionレイヤの使用方法\n",
        "+ Attentionレイヤをどこに置くかで精度が変わるが、やってみないとわからない。\n",
        " + AttentionレイヤをLSTMレイヤとAffineレイヤの間に入れるケース\n",
        " + 次時刻のLSTMレイヤとの間に入れるケース"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4dJWKNuJQ8J",
        "colab_type": "text"
      },
      "source": [
        "### seq2seqの深層化とskipコネクション\n",
        "+ 深層化\n",
        " + 層を重ねることで、表現力の高いモデルを作ることができる。\n",
        "   + 層を重ねた場合、複数のレイヤにAttentionレイヤによるコンテキストベクトルを渡していく。\n",
        "+ skipコネクション\n",
        " + 層をまたいで線を繋ぐ（加算）というシンプルなテクニック\n",
        "   + 逆伝播時には勾配をそのまま流すだけになる。勾配が消失あるいは爆発しない。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wQBSlPPLIzR",
        "colab_type": "text"
      },
      "source": [
        "## Attentionの応用"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDFr1dQVLO8j",
        "colab_type": "text"
      },
      "source": [
        "### Google Neural Machine Translation(GNMT)\n",
        "+ LSTMレイヤの多層化\n",
        "+ skipコネクション\n",
        "+ 複数GPUでの分散学習"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkB5uSNGL96W",
        "colab_type": "text"
      },
      "source": [
        "### Transformer\n",
        "+ RNNの欠点は並列化処理\n",
        " + 前の時間に計算した結果を使って逐次的に計算するため、時間方向で並列化することはできない。\n",
        " + RNNを使わないようにする研究があるらしい。\n",
        "  + 代わりにCNNを使ったり、Attentionを使ったり\n",
        "\n",
        "\n",
        "+ TransformerはAttentionによって構成される。\n",
        " + Self-Attention（自分自身に対してのAttention）\n",
        "  + ひとつの時系列データ内において各要素が他の要素に対してどのような関連性があるのかを見ていこうというもの。\n",
        " + 計算量を抑え、GPUによる並列計算の恩恵をより多くうけることができる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPi3hO_eFeF8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}